{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CÓDIGO PARA COMPARAÇÃO SELF ASSESSMENT X AVALIAÇÃO DOS JURADOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACURACIA WINNERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft voting CSV saved to: soft_voting.csv\n",
      "Hard voting CSV saved to: hard_voting.csv\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Placeholder paths for JSON input and CSV outputs\n",
    "input_json_path = \"aggregated_debate_results.json\"\n",
    "soft_voting_csv_path = \"soft_voting.csv\"\n",
    "hard_voting_csv_path = \"hard_voting.csv\"\n",
    "\n",
    "# Load JSON data\n",
    "with open(input_json_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Prepare lists for each CSV\n",
    "soft_voting_rows = []\n",
    "hard_voting_rows = []\n",
    "\n",
    "# Process the JSON data to extract rows for both CSVs\n",
    "for debate in data:\n",
    "    debate_id = debate[\"debate_id\"]\n",
    "    for entry in debate[\"soft_voting_ranking\"]:\n",
    "        soft_voting_rows.append(\n",
    "            {\"debate_id\": debate_id, \"position\": entry[\"position\"], \"name\": entry[\"name\"], \"score\": entry[\"score\"]}\n",
    "        )\n",
    "    for entry in debate[\"hard_voting_ranking\"]:\n",
    "        hard_voting_rows.append(\n",
    "            {\"debate_id\": debate_id, \"position\": entry[\"position\"], \"name\": entry[\"name\"], \"score\": entry[\"score\"]}\n",
    "        )\n",
    "\n",
    "# Create DataFrames\n",
    "soft_voting_df = pd.DataFrame(soft_voting_rows)\n",
    "hard_voting_df = pd.DataFrame(hard_voting_rows)\n",
    "\n",
    "# Save to CSV files\n",
    "soft_voting_df.to_csv(soft_voting_csv_path, index=False)\n",
    "hard_voting_df.to_csv(hard_voting_csv_path, index=False)\n",
    "\n",
    "print(f\"Soft voting CSV saved to: {soft_voting_csv_path}\")\n",
    "print(f\"Hard voting CSV saved to: {hard_voting_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Hard Voting:\n",
      "Accuracy of winners (hard voting): 71.43%\n",
      "Debate ID: 1\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 2\n",
      "Expected winners: Debater 4, Debater 1\n",
      "Predicted winners: Debater 5\n",
      "----------------------------------------\n",
      "Debate ID: 3\n",
      "Expected winners: Debater 4\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 5\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 5\n",
      "----------------------------------------\n",
      "Debate ID: 6\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 7\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 8\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 9\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 4, Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 11\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 5\n",
      "----------------------------------------\n",
      "Debate ID: 12\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 13\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 14\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 16\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 18\n",
      "Expected winners: Debater 2, Debater 3, Debater 1\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "\n",
      "Evaluating Soft Voting:\n",
      "Accuracy of winners (soft voting): 64.29%\n",
      "Debate ID: 1\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 2\n",
      "Expected winners: Debater 4, Debater 1\n",
      "Predicted winners: Debater 5\n",
      "----------------------------------------\n",
      "Debate ID: 3\n",
      "Expected winners: Debater 4\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 5\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 5\n",
      "----------------------------------------\n",
      "Debate ID: 6\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 7\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 8\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 9\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 11\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 5\n",
      "----------------------------------------\n",
      "Debate ID: 12\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 13\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 14\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 16\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 18\n",
      "Expected winners: Debater 2, Debater 3, Debater 1\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def evaluate_predictions(ground_truth_path, predictions_path, excluded_debates, voting_type):\n",
    "    \"\"\"\n",
    "    Avalia a precisão das predições com base nos vencedores previstos e reais.\n",
    "    \n",
    "    Args:\n",
    "        ground_truth_path (str): Caminho para o arquivo CSV de ground truth (self-assessment).\n",
    "        predictions_path (str): Caminho para o arquivo CSV das predições (hard ou soft voting).\n",
    "        excluded_debates (list): Lista de IDs de debates a serem excluídos.\n",
    "        voting_type (str): Tipo de votação ('hard' ou 'soft') para indicar o tipo de predictions.\n",
    "    \n",
    "    Returns:\n",
    "        None: Exibe a precisão calculada e detalhes.\n",
    "    \"\"\"\n",
    "    # Load datasets\n",
    "    ground_truth_df = pd.read_csv(ground_truth_path)\n",
    "    predictions_df = pd.read_csv(predictions_path)\n",
    "\n",
    "    # Exclude specific debates\n",
    "    ground_truth_df = ground_truth_df[~ground_truth_df['debate_id'].isin(excluded_debates)]\n",
    "    predictions_df = predictions_df[~predictions_df['debate_id'].isin(excluded_debates)]\n",
    "\n",
    "    # Rename columns to ensure consistency\n",
    "    ground_truth_df = ground_truth_df.rename(columns={\n",
    "        'debater_name': 'name',\n",
    "        'debater_position': 'position'\n",
    "    })\n",
    "\n",
    "    # Merge datasets on debate_id\n",
    "    merged_df = pd.merge(\n",
    "        ground_truth_df[['debate_id', 'name', 'position']],\n",
    "        predictions_df[['debate_id', 'name', 'position']],\n",
    "        on='debate_id',\n",
    "        suffixes=('_true', '_pred')\n",
    "    )\n",
    "\n",
    "    # Identify ground truth winners\n",
    "    ground_truth_winners = merged_df[merged_df['position_true'] == 1]\n",
    "\n",
    "    # Initialize list for accuracy calculations and details\n",
    "    correct_predictions = []\n",
    "    debate_details = []\n",
    "\n",
    "    for debate_id, group in ground_truth_winners.groupby('debate_id'):\n",
    "        # Get the names of the ground truth winners\n",
    "        true_winners = set(group['name_true'])\n",
    "        \n",
    "        # Get the predicted winners for this debate_id\n",
    "        predicted_winners = set(\n",
    "            merged_df[\n",
    "                (merged_df['debate_id'] == debate_id) & \n",
    "                (merged_df['position_pred'] == 1)\n",
    "            ]['name_pred']\n",
    "        )\n",
    "        \n",
    "        # Save the details for display\n",
    "        debate_details.append({\n",
    "            'debate_id': debate_id,\n",
    "            'true_winners': true_winners,\n",
    "            'predicted_winners': predicted_winners\n",
    "        })\n",
    "        \n",
    "        # If there is an intersection between true and predicted winners, it's a correct prediction\n",
    "        if true_winners.intersection(predicted_winners):\n",
    "            correct_predictions.append(1)\n",
    "        else:\n",
    "            correct_predictions.append(0)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = sum(correct_predictions) / len(correct_predictions)\n",
    "    print(f'Accuracy of winners ({voting_type} voting): {accuracy * 100:.2f}%')\n",
    "\n",
    "    # Print the expected and predicted winners for each debate\n",
    "    for detail in debate_details:\n",
    "        print(f\"Debate ID: {detail['debate_id']}\")\n",
    "        print(f\"Expected winners: {', '.join(detail['true_winners'])}\")\n",
    "        print(f\"Predicted winners: {', '.join(detail['predicted_winners'])}\")\n",
    "        print('-' * 40)\n",
    "\n",
    "# File paths\n",
    "hard_voting_path = 'hard_voting.csv'\n",
    "soft_voting_path = 'soft_voting.csv'\n",
    "self_assessment_path = 'rankings_self_assessment_.csv'\n",
    "\n",
    "# Excluded debates\n",
    "excluded_debates = [10, 17]\n",
    "\n",
    "# Evaluate for hard voting\n",
    "print(\"Evaluating Hard Voting:\")\n",
    "evaluate_predictions(self_assessment_path, hard_voting_path, excluded_debates, 'hard')\n",
    "\n",
    "# Evaluate for soft voting\n",
    "print(\"\\nEvaluating Soft Voting:\")\n",
    "evaluate_predictions(self_assessment_path, soft_voting_path, excluded_debates, 'soft')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACURACIA DEBATERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Avaliando Hard Voting ---\n",
      "\n",
      "Debate ID: 1\n",
      "Ranking esperado: Debater 2, Debater 3, Debater 1, Debater 4\n",
      "Ranking previsto: Debater 2, Debater 4, Debater 1, Debater 3\n",
      "2 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 2\n",
      "Ranking esperado: Debater 5, Debater 4, Debater 1, Debater 2, Debater 3\n",
      "Ranking previsto: Debater 1, Debater 4, Debater 5, Debater 2, Debater 3\n",
      "3 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 3\n",
      "Ranking esperado: Debater 3, Debater 4, Debater 1, Debater 2, Debater 5\n",
      "Ranking previsto: Debater 4, Debater 3, Debater 1, Debater 2, Debater 5\n",
      "3 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 5\n",
      "Ranking esperado: Debater 5, Debater 3, Debater 1, Debater 2, Debater 4\n",
      "Ranking previsto: Debater 3, Debater 5, Debater 1, Debater 2, Debater 4\n",
      "3 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 6\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "4 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 7\n",
      "Ranking esperado: Debater 2, Debater 3, Debater 1, Debater 4\n",
      "Ranking previsto: Debater 2, Debater 1, Debater 3, Debater 4\n",
      "2 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 8\n",
      "Ranking esperado: Debater 1, Debater 3, Debater 2\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3\n",
      "1 acertos de 3 debatedores.\n",
      "\n",
      "Debate ID: 9\n",
      "Ranking esperado: Debater 1, Debater 4, Debater 2, Debater 3\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "1 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 11\n",
      "Ranking esperado: Debater 5, Debater 3, Debater 1, Debater 2, Debater 4\n",
      "Ranking previsto: Debater 3, Debater 1, Debater 2, Debater 4, Debater 5\n",
      "0 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 12\n",
      "Ranking esperado: Debater 3, Debater 4, Debater 1, Debater 2\n",
      "Ranking previsto: Debater 3, Debater 1, Debater 2, Debater 4\n",
      "1 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 13\n",
      "Ranking esperado: Debater 3, Debater 2, Debater 1\n",
      "Ranking previsto: Debater 3, Debater 1, Debater 2\n",
      "1 acertos de 3 debatedores.\n",
      "\n",
      "Debate ID: 14\n",
      "Ranking esperado: Debater 2, Debater 5, Debater 1, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 2, Debater 1, Debater 3, Debater 4, Debater 5\n",
      "1 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 16\n",
      "Ranking esperado: Debater 3, Debater 2, Debater 4, Debater 1, Debater 5\n",
      "Ranking previsto: Debater 3, Debater 2, Debater 1, Debater 4, Debater 5\n",
      "3 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 18\n",
      "Ranking esperado: Debater 3, Debater 1, Debater 2, Debater 4\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "1 acertos de 4 debatedores.\n",
      "\n",
      "Acurácia de ranking: 42.98%\n",
      "Total de acertos: 26 de 60 debatedores (43.33%)\n",
      "\n",
      "--- Avaliando Soft Voting ---\n",
      "\n",
      "Debate ID: 1\n",
      "Ranking esperado: Debater 2, Debater 3, Debater 1, Debater 4\n",
      "Ranking previsto: Debater 2, Debater 4, Debater 1, Debater 3\n",
      "2 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 2\n",
      "Ranking esperado: Debater 5, Debater 4, Debater 3, Debater 1, Debater 2\n",
      "Ranking previsto: Debater 1, Debater 4, Debater 5, Debater 2, Debater 3\n",
      "1 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 3\n",
      "Ranking esperado: Debater 3, Debater 4, Debater 1, Debater 5, Debater 2\n",
      "Ranking previsto: Debater 4, Debater 3, Debater 1, Debater 2, Debater 5\n",
      "1 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 5\n",
      "Ranking esperado: Debater 5, Debater 3, Debater 4, Debater 2, Debater 1\n",
      "Ranking previsto: Debater 3, Debater 5, Debater 1, Debater 2, Debater 4\n",
      "1 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 6\n",
      "Ranking esperado: Debater 1, Debater 3, Debater 2, Debater 4\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "2 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 7\n",
      "Ranking esperado: Debater 3, Debater 2, Debater 4, Debater 1\n",
      "Ranking previsto: Debater 2, Debater 1, Debater 3, Debater 4\n",
      "0 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 8\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3\n",
      "3 acertos de 3 debatedores.\n",
      "\n",
      "Debate ID: 9\n",
      "Ranking esperado: Debater 1, Debater 4, Debater 2, Debater 3\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "1 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 11\n",
      "Ranking esperado: Debater 5, Debater 3, Debater 1, Debater 2, Debater 4\n",
      "Ranking previsto: Debater 3, Debater 1, Debater 2, Debater 4, Debater 5\n",
      "0 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 12\n",
      "Ranking esperado: Debater 3, Debater 4, Debater 1, Debater 2\n",
      "Ranking previsto: Debater 3, Debater 1, Debater 2, Debater 4\n",
      "1 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 13\n",
      "Ranking esperado: Debater 3, Debater 2, Debater 1\n",
      "Ranking previsto: Debater 3, Debater 1, Debater 2\n",
      "1 acertos de 3 debatedores.\n",
      "\n",
      "Debate ID: 14\n",
      "Ranking esperado: Debater 2, Debater 5, Debater 4, Debater 3, Debater 1\n",
      "Ranking previsto: Debater 2, Debater 1, Debater 3, Debater 4, Debater 5\n",
      "1 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 16\n",
      "Ranking esperado: Debater 3, Debater 2, Debater 5, Debater 4, Debater 1\n",
      "Ranking previsto: Debater 3, Debater 2, Debater 1, Debater 4, Debater 5\n",
      "3 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 18\n",
      "Ranking esperado: Debater 2, Debater 3, Debater 1, Debater 4\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "1 acertos de 4 debatedores.\n",
      "\n",
      "Acurácia de ranking: 32.02%\n",
      "Total de acertos: 18 de 60 debatedores (30.00%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_ranking_accuracy(ground_truth_df, predictions_df, excluded_debates=[10, 17]):\n",
    "    \"\"\"\n",
    "    Calcula a acurácia do ranking comparando as previsões de rankings com os rankings reais (ground truth).\n",
    "\n",
    "    Parameters:\n",
    "    - ground_truth_df: DataFrame com os rankings reais (hard ou soft voting).\n",
    "    - predictions_df: DataFrame com as previsões dos modelos (ex: self-assessment).\n",
    "    - excluded_debates: Lista de debates a serem excluídos da análise (default: [10, 17]).\n",
    "\n",
    "    Returns:\n",
    "    - ranking_accuracy: Acurácia do ranking (em porcentagem).\n",
    "    \"\"\"\n",
    "\n",
    "    # Filtra os debates a serem excluídos\n",
    "    ground_truth_df = ground_truth_df[~ground_truth_df['debate_id'].isin(excluded_debates)]\n",
    "    predictions_df = predictions_df[~predictions_df['debate_id'].isin(excluded_debates)]\n",
    "\n",
    "    # Lista para armazenar as acurácias de ranking de cada debate\n",
    "    correct_rankings = []\n",
    "    debate_details = []\n",
    "\n",
    "    # Variáveis para acumular o número total de acertos e debatedores\n",
    "    total_correct = 0\n",
    "    total_debaters = 0\n",
    "\n",
    "    # Para cada debate, compara o ranking completo dos debatedores\n",
    "    for debate_id in ground_truth_df['debate_id'].unique():\n",
    "        # Filtra os debatedores do debate real e do modelo\n",
    "        true_ranking = ground_truth_df[ground_truth_df['debate_id'] == debate_id][['name', 'position']]\n",
    "        predicted_ranking = predictions_df[predictions_df['debate_id'] == debate_id][['debater_name', 'debater_position']]\n",
    "\n",
    "        # Ordena os debatedores conforme suas posições no ranking (ground truth e previsão)\n",
    "        true_ranking_sorted = true_ranking.sort_values(by='position').reset_index(drop=True)\n",
    "        predicted_ranking_sorted = predicted_ranking.sort_values(by='debater_position').reset_index(drop=True)\n",
    "\n",
    "        # Salva os detalhes do debate para exibição\n",
    "        debate_details.append({\n",
    "            'debate_id': debate_id,\n",
    "            'true_ranking': true_ranking_sorted['name'].tolist(),\n",
    "            'predicted_ranking': predicted_ranking_sorted['debater_name'].tolist()\n",
    "        })\n",
    "\n",
    "        # Inicializa o contador de acertos considerando empates\n",
    "        correct_rankings_count = 0\n",
    "\n",
    "        # Compara os rankings completos (considerando a ordem exata de posições)\n",
    "        for i in range(len(true_ranking_sorted)):\n",
    "            if true_ranking_sorted.iloc[i]['name'] == predicted_ranking_sorted.iloc[i]['debater_name']:\n",
    "                correct_rankings_count += 1\n",
    "\n",
    "        # Calcula a fração de debatedores classificados corretamente neste debate\n",
    "        correct_rankings.append(correct_rankings_count / len(true_ranking_sorted))\n",
    "\n",
    "        # Acumula os valores totais\n",
    "        total_correct += correct_rankings_count\n",
    "        total_debaters += len(true_ranking_sorted)\n",
    "\n",
    "        # Exibe informações do debate\n",
    "        print(f\"Debate ID: {debate_id}\")\n",
    "        print(f\"Ranking esperado: {', '.join(true_ranking_sorted['name'])}\")\n",
    "        print(f\"Ranking previsto: {', '.join(predicted_ranking_sorted['debater_name'])}\")\n",
    "        print(f\"{correct_rankings_count} acertos de {len(true_ranking_sorted)} debatedores.\\n\")\n",
    "\n",
    "    # Calcula a acurácia média de ranking\n",
    "    ranking_accuracy = sum(correct_rankings) / len(correct_rankings) if correct_rankings else 0\n",
    "\n",
    "    # Exibe a acurácia do ranking\n",
    "    print(f'Acurácia de ranking: {ranking_accuracy * 100:.2f}%')\n",
    "\n",
    "    # Exibe o número total de acertos dividido pelo número total de debatedores\n",
    "    print(f\"Total de acertos: {total_correct} de {total_debaters} debatedores ({(total_correct / total_debaters) * 100:.2f}%)\")\n",
    "\n",
    "    return ranking_accuracy\n",
    "\n",
    "# Função principal para avaliar hard e soft voting\n",
    "def evaluate_voting(ground_truth_path, predictions_path, excluded_debates, voting_type):\n",
    "    print(f\"\\n--- Avaliando {voting_type.capitalize()} Voting ---\\n\")\n",
    "\n",
    "    # Carregar os datasets\n",
    "    ground_truth_df = pd.read_csv(ground_truth_path)\n",
    "    predictions_df = pd.read_csv(predictions_path)\n",
    "\n",
    "    # Ajustar nomes das colunas para uniformizar\n",
    "    ground_truth_df.rename(columns={'position': 'position', 'name': 'name'}, inplace=True)\n",
    "    predictions_df.rename(columns={'debater_position': 'debater_position', 'debater_name': 'debater_name'}, inplace=True)\n",
    "\n",
    "    # Calcular acurácia do ranking\n",
    "    calculate_ranking_accuracy(ground_truth_df, predictions_df, excluded_debates)\n",
    "\n",
    "# Caminhos para os arquivos\n",
    "hard_voting_path = \"hard_voting.csv\"\n",
    "soft_voting_path = \"soft_voting.csv\"\n",
    "self_assessment_path = \"rankings_self_assessment_.csv\"\n",
    "\n",
    "# Excluir debates\n",
    "excluded_debates = [10, 17]\n",
    "\n",
    "# Avaliar para hard voting\n",
    "evaluate_voting(hard_voting_path, self_assessment_path, excluded_debates, 'hard')\n",
    "\n",
    "# Avaliar para soft voting\n",
    "evaluate_voting(soft_voting_path, self_assessment_path, excluded_debates, 'soft')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_mrr(hard_voting_df, soft_voting_df, self_assessment_df, excluded_debates=[10, 17]):\n",
    "    \"\"\"\n",
    "    Calcula o Mean Reciprocal Rank (MRR) comparando as previsões de rankings com os rankings reais (ground truth).\n",
    "    \n",
    "    Parameters:\n",
    "    - hard_voting_df: DataFrame com as previsões dos votos duros (hard voting).\n",
    "    - soft_voting_df: DataFrame com as previsões dos votos suaves (soft voting).\n",
    "    - self_assessment_df: DataFrame com as autoavaliações (previsões).\n",
    "    - excluded_debates: Lista de debates a serem excluídos da análise (default: [10, 17]).\n",
    "    \n",
    "    Returns:\n",
    "    - mrr_hard: Mean Reciprocal Rank (em decimal) para hard voting.\n",
    "    - mrr_soft: Mean Reciprocal Rank (em decimal) para soft voting.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filtra os debates a serem excluídos\n",
    "    hard_voting_df = hard_voting_df[~hard_voting_df['debate_id'].isin(excluded_debates)]\n",
    "    soft_voting_df = soft_voting_df[~soft_voting_df['debate_id'].isin(excluded_debates)]\n",
    "    self_assessment_df = self_assessment_df[~self_assessment_df['debate_id'].isin(excluded_debates)]\n",
    "\n",
    "    # Lista para armazenar os Reciprocal Ranks de cada debate\n",
    "    reciprocal_ranks_hard = []\n",
    "    reciprocal_ranks_soft = []\n",
    "\n",
    "    # Para cada debate, calcula o Reciprocal Rank para hard e soft voting\n",
    "    for debate_id in self_assessment_df['debate_id'].unique():\n",
    "        # Filtra os debatedores do debate real (hard e soft voting)\n",
    "        true_ranking_hard = hard_voting_df[hard_voting_df['debate_id'] == debate_id][['name', 'position']]\n",
    "        true_ranking_soft = soft_voting_df[soft_voting_df['debate_id'] == debate_id][['name', 'position']]\n",
    "        \n",
    "        # Filtra os rankings de predições (self-assessment)\n",
    "        predicted_ranking = self_assessment_df[self_assessment_df['debate_id'] == debate_id][['debater_name', 'debater_position']]\n",
    "\n",
    "        # Ordena os debatedores conforme suas posições no ranking (ground truth e previsões)\n",
    "        true_ranking_hard_sorted = true_ranking_hard.sort_values(by='position').reset_index(drop=True)\n",
    "        true_ranking_soft_sorted = true_ranking_soft.sort_values(by='position').reset_index(drop=True)\n",
    "        predicted_ranking_sorted = predicted_ranking.sort_values(by='debater_position').reset_index(drop=True)\n",
    "\n",
    "        # Lista de vencedores no ground truth (pode haver empate)\n",
    "        true_winners_hard = set(true_ranking_hard_sorted[true_ranking_hard_sorted['position'] == 1]['name'])\n",
    "        true_winners_soft = set(true_ranking_soft_sorted[true_ranking_soft_sorted['position'] == 1]['name'])\n",
    "\n",
    "        # Calcula o Reciprocal Rank para hard voting\n",
    "        rr_hard = 0\n",
    "        for rank, debater in enumerate(predicted_ranking_sorted['debater_name'], start=1):\n",
    "            if debater in true_winners_hard:\n",
    "                rr_hard = 1 / rank\n",
    "                break\n",
    "        \n",
    "        # Calcula o Reciprocal Rank para soft voting\n",
    "        rr_soft = 0\n",
    "        for rank, debater in enumerate(predicted_ranking_sorted['debater_name'], start=1):\n",
    "            if debater in true_winners_soft:\n",
    "                rr_soft = 1 / rank\n",
    "                break\n",
    "        \n",
    "        # Armazena os RRs dos debates\n",
    "        reciprocal_ranks_hard.append(rr_hard)\n",
    "        reciprocal_ranks_soft.append(rr_soft)\n",
    "\n",
    "        # Exibe informações do debate\n",
    "        print(f\"Debate ID: {debate_id}\")\n",
    "        print(f\"Vencedores esperados (Hard Voting): {', '.join(true_winners_hard)}\")\n",
    "        print(f\"Vencedores esperados (Soft Voting): {', '.join(true_winners_soft)}\")\n",
    "        print(f\"Ranking previsto (Self Assessment): {', '.join(predicted_ranking_sorted['debater_name'])}\")\n",
    "        print(f\"Reciprocal Rank (Hard Voting): {rr_hard:.3f}\")\n",
    "        print(f\"Reciprocal Rank (Soft Voting): {rr_soft:.3f}\\n\")\n",
    "\n",
    "    # Calcula o MRR para hard e soft voting\n",
    "    mrr_hard = sum(reciprocal_ranks_hard) / len(reciprocal_ranks_hard) if reciprocal_ranks_hard else 0\n",
    "    mrr_soft = sum(reciprocal_ranks_soft) / len(reciprocal_ranks_soft) if reciprocal_ranks_soft else 0\n",
    "    \n",
    "    # Exibe o MRR\n",
    "    print(f\"Mean Reciprocal Rank (MRR) - Hard Voting: {mrr_hard:.3f}\")\n",
    "    print(f\"Mean Reciprocal Rank (MRR) - Soft Voting: {mrr_soft:.3f}\")\n",
    "\n",
    "    return mrr_hard, mrr_soft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debate ID: 1\n",
      "Vencedores esperados (Hard Voting): Debater 2\n",
      "Vencedores esperados (Soft Voting): Debater 2\n",
      "Ranking previsto (Self Assessment): Debater 2, Debater 4, Debater 1, Debater 3\n",
      "Reciprocal Rank (Hard Voting): 1.000\n",
      "Reciprocal Rank (Soft Voting): 1.000\n",
      "\n",
      "Debate ID: 2\n",
      "Vencedores esperados (Hard Voting): Debater 5\n",
      "Vencedores esperados (Soft Voting): Debater 5\n",
      "Ranking previsto (Self Assessment): Debater 1, Debater 4, Debater 5, Debater 2, Debater 3\n",
      "Reciprocal Rank (Hard Voting): 0.333\n",
      "Reciprocal Rank (Soft Voting): 0.333\n",
      "\n",
      "Debate ID: 3\n",
      "Vencedores esperados (Hard Voting): Debater 3\n",
      "Vencedores esperados (Soft Voting): Debater 3\n",
      "Ranking previsto (Self Assessment): Debater 4, Debater 3, Debater 1, Debater 2, Debater 5\n",
      "Reciprocal Rank (Hard Voting): 0.500\n",
      "Reciprocal Rank (Soft Voting): 0.500\n",
      "\n",
      "Debate ID: 5\n",
      "Vencedores esperados (Hard Voting): Debater 5\n",
      "Vencedores esperados (Soft Voting): Debater 5\n",
      "Ranking previsto (Self Assessment): Debater 3, Debater 5, Debater 1, Debater 2, Debater 4\n",
      "Reciprocal Rank (Hard Voting): 0.500\n",
      "Reciprocal Rank (Soft Voting): 0.500\n",
      "\n",
      "Debate ID: 6\n",
      "Vencedores esperados (Hard Voting): Debater 1\n",
      "Vencedores esperados (Soft Voting): Debater 1\n",
      "Ranking previsto (Self Assessment): Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Reciprocal Rank (Hard Voting): 1.000\n",
      "Reciprocal Rank (Soft Voting): 1.000\n",
      "\n",
      "Debate ID: 7\n",
      "Vencedores esperados (Hard Voting): Debater 2\n",
      "Vencedores esperados (Soft Voting): Debater 3\n",
      "Ranking previsto (Self Assessment): Debater 2, Debater 1, Debater 3, Debater 4\n",
      "Reciprocal Rank (Hard Voting): 1.000\n",
      "Reciprocal Rank (Soft Voting): 0.333\n",
      "\n",
      "Debate ID: 8\n",
      "Vencedores esperados (Hard Voting): Debater 1\n",
      "Vencedores esperados (Soft Voting): Debater 1\n",
      "Ranking previsto (Self Assessment): Debater 1, Debater 2, Debater 3\n",
      "Reciprocal Rank (Hard Voting): 1.000\n",
      "Reciprocal Rank (Soft Voting): 1.000\n",
      "\n",
      "Debate ID: 9\n",
      "Vencedores esperados (Hard Voting): Debater 4, Debater 1\n",
      "Vencedores esperados (Soft Voting): Debater 1\n",
      "Ranking previsto (Self Assessment): Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Reciprocal Rank (Hard Voting): 1.000\n",
      "Reciprocal Rank (Soft Voting): 1.000\n",
      "\n",
      "Debate ID: 11\n",
      "Vencedores esperados (Hard Voting): Debater 5\n",
      "Vencedores esperados (Soft Voting): Debater 5\n",
      "Ranking previsto (Self Assessment): Debater 3, Debater 1, Debater 2, Debater 4, Debater 5\n",
      "Reciprocal Rank (Hard Voting): 0.200\n",
      "Reciprocal Rank (Soft Voting): 0.200\n",
      "\n",
      "Debate ID: 12\n",
      "Vencedores esperados (Hard Voting): Debater 3\n",
      "Vencedores esperados (Soft Voting): Debater 3\n",
      "Ranking previsto (Self Assessment): Debater 3, Debater 1, Debater 2, Debater 4\n",
      "Reciprocal Rank (Hard Voting): 1.000\n",
      "Reciprocal Rank (Soft Voting): 1.000\n",
      "\n",
      "Debate ID: 13\n",
      "Vencedores esperados (Hard Voting): Debater 3\n",
      "Vencedores esperados (Soft Voting): Debater 3\n",
      "Ranking previsto (Self Assessment): Debater 3, Debater 1, Debater 2\n",
      "Reciprocal Rank (Hard Voting): 1.000\n",
      "Reciprocal Rank (Soft Voting): 1.000\n",
      "\n",
      "Debate ID: 14\n",
      "Vencedores esperados (Hard Voting): Debater 2\n",
      "Vencedores esperados (Soft Voting): Debater 2\n",
      "Ranking previsto (Self Assessment): Debater 2, Debater 1, Debater 3, Debater 4, Debater 5\n",
      "Reciprocal Rank (Hard Voting): 1.000\n",
      "Reciprocal Rank (Soft Voting): 1.000\n",
      "\n",
      "Debate ID: 16\n",
      "Vencedores esperados (Hard Voting): Debater 3\n",
      "Vencedores esperados (Soft Voting): Debater 3\n",
      "Ranking previsto (Self Assessment): Debater 3, Debater 2, Debater 1, Debater 4, Debater 5\n",
      "Reciprocal Rank (Hard Voting): 1.000\n",
      "Reciprocal Rank (Soft Voting): 1.000\n",
      "\n",
      "Debate ID: 18\n",
      "Vencedores esperados (Hard Voting): Debater 3\n",
      "Vencedores esperados (Soft Voting): Debater 2\n",
      "Ranking previsto (Self Assessment): Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Reciprocal Rank (Hard Voting): 0.333\n",
      "Reciprocal Rank (Soft Voting): 0.500\n",
      "\n",
      "Mean Reciprocal Rank (MRR) - Hard Voting: 0.776\n",
      "Mean Reciprocal Rank (MRR) - Soft Voting: 0.740\n",
      "MRR - Hard Voting: 0.776\n",
      "MRR - Soft Voting: 0.740\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carregar os arquivos CSV\n",
    "self_assessment_df = pd.read_csv('rankings_self_assessment_.csv')\n",
    "hard_voting_df = pd.read_csv('hard_voting.csv')\n",
    "soft_voting_df = pd.read_csv('soft_voting.csv')\n",
    "\n",
    "# Chamar a função para calcular o MRR\n",
    "mrr_hard, mrr_soft = calculate_mrr(hard_voting_df, soft_voting_df, self_assessment_df)\n",
    "\n",
    "# Exibir os resultados\n",
    "print(f\"MRR - Hard Voting: {mrr_hard:.3f}\")\n",
    "print(f\"MRR - Soft Voting: {mrr_soft:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating nDCG for Hard Voting...\n",
      "Debate ID: 1\n",
      "True Relevance (original): [0 3 2 0]\n",
      "Predicted Order: ['Debater 2' 'Debater 4' 'Debater 1' 'Debater 3']\n",
      "Reordered True Relevance: [np.int64(3), np.int64(0), np.int64(0), np.int64(2)]\n",
      "nDCG for Debate 1: 0.546\n",
      "\n",
      "Debate ID: 2\n",
      "True Relevance (original): [0 0 0 2 5]\n",
      "Predicted Order: ['Debater 1' 'Debater 4' 'Debater 5' 'Debater 2' 'Debater 3']\n",
      "Reordered True Relevance: [np.int64(0), np.int64(2), np.int64(5), np.int64(0), np.int64(0)]\n",
      "nDCG for Debate 2: 0.491\n",
      "\n",
      "Debate ID: 3\n",
      "True Relevance (original): [1 0 3 2 0]\n",
      "Predicted Order: ['Debater 4' 'Debater 3' 'Debater 2' 'Debater 1' 'Debater 5']\n",
      "Reordered True Relevance: [np.int64(2), np.int64(3), np.int64(0), np.int64(1), np.int64(0)]\n",
      "nDCG for Debate 3: 0.600\n",
      "\n",
      "Debate ID: 5\n",
      "True Relevance (original): [0 0 2 0 4]\n",
      "Predicted Order: ['Debater 3' 'Debater 5' 'Debater 2' 'Debater 1' 'Debater 4']\n",
      "Reordered True Relevance: [np.int64(2), np.int64(4), np.int64(0), np.int64(0), np.int64(0)]\n",
      "nDCG for Debate 5: 0.501\n",
      "\n",
      "Debate ID: 6\n",
      "True Relevance (original): [4 2 2 0]\n",
      "Predicted Order: ['Debater 1' 'Debater 2' 'Debater 3' 'Debater 4']\n",
      "Reordered True Relevance: [np.int64(4), np.int64(2), np.int64(2), np.int64(0)]\n",
      "nDCG for Debate 6: 1.000\n",
      "\n",
      "Debate ID: 7\n",
      "True Relevance (original): [1 4 2 1]\n",
      "Predicted Order: ['Debater 2' 'Debater 1' 'Debater 3' 'Debater 4']\n",
      "Reordered True Relevance: [np.int64(4), np.int64(1), np.int64(2), np.int64(1)]\n",
      "nDCG for Debate 7: 0.741\n",
      "\n",
      "Debate ID: 8\n",
      "True Relevance (original): [5 0 1]\n",
      "Predicted Order: ['Debater 1' 'Debater 2' 'Debater 3']\n",
      "Reordered True Relevance: [np.int64(5), np.int64(0), np.int64(1)]\n",
      "nDCG for Debate 8: 1.000\n",
      "\n",
      "Debate ID: 9\n",
      "True Relevance (original): [3 0 0 3]\n",
      "Predicted Order: ['Debater 1' 'Debater 2' 'Debater 3' 'Debater 4']\n",
      "Reordered True Relevance: [np.int64(3), np.int64(0), np.int64(0), np.int64(3)]\n",
      "nDCG for Debate 9: 1.000\n",
      "\n",
      "Debate ID: 11\n",
      "True Relevance (original): [0 0 2 0 5]\n",
      "Predicted Order: ['Debater 3' 'Debater 1' 'Debater 2' 'Debater 4' 'Debater 5']\n",
      "Reordered True Relevance: [np.int64(2), np.int64(0), np.int64(0), np.int64(0), np.int64(5)]\n",
      "nDCG for Debate 11: 0.939\n",
      "\n",
      "Debate ID: 12\n",
      "True Relevance (original): [0 0 4 2]\n",
      "Predicted Order: ['Debater 3' 'Debater 1' 'Debater 2' 'Debater 4']\n",
      "Reordered True Relevance: [np.int64(4), np.int64(0), np.int64(0), np.int64(2)]\n",
      "nDCG for Debate 12: 0.594\n",
      "\n",
      "Debate ID: 13\n",
      "True Relevance (original): [0 3 4]\n",
      "Predicted Order: ['Debater 3' 'Debater 1' 'Debater 2']\n",
      "Reordered True Relevance: [np.int64(4), np.int64(0), np.int64(3)]\n",
      "nDCG for Debate 13: 0.683\n",
      "\n",
      "Debate ID: 14\n",
      "True Relevance (original): [0 4 0 0 3]\n",
      "Predicted Order: ['Debater 2' 'Debater 1' 'Debater 3' 'Debater 4' 'Debater 5']\n",
      "Reordered True Relevance: [np.int64(4), np.int64(0), np.int64(0), np.int64(0), np.int64(3)]\n",
      "nDCG for Debate 14: 0.619\n",
      "\n",
      "Debate ID: 16\n",
      "True Relevance (original): [0 1 3 1 0]\n",
      "Predicted Order: ['Debater 3' 'Debater 2' 'Debater 1' 'Debater 4' 'Debater 5']\n",
      "Reordered True Relevance: [np.int64(3), np.int64(1), np.int64(0), np.int64(1), np.int64(0)]\n",
      "nDCG for Debate 16: 0.571\n",
      "\n",
      "Debate ID: 18\n",
      "True Relevance (original): [2 2 3 1]\n",
      "Predicted Order: ['Debater 1' 'Debater 2' 'Debater 3' 'Debater 4']\n",
      "Reordered True Relevance: [np.int64(2), np.int64(2), np.int64(3), np.int64(1)]\n",
      "nDCG for Debate 18: 1.000\n",
      "\n",
      "All nDCG Scores: [np.float64(0.5459333868899072), np.float64(0.4909460492065861), np.float64(0.6000227006377337), np.float64(0.500784699218747), np.float64(1.0), np.float64(0.7409808928042337), np.float64(1.0), np.float64(0.9999999999999998), np.float64(0.9387551578728078), np.float64(0.5935568251204683), np.float64(0.6828207893180352), np.float64(0.6193153903288456), np.float64(0.5706279075540768), np.float64(1.0)]\n",
      "Mean nDCG: 0.735\n",
      "\n",
      "Calculating nDCG for Soft Voting...\n",
      "Debate ID: 1\n",
      "True Relevance (original): [124 195 183 124]\n",
      "Predicted Order: ['Debater 2' 'Debater 4' 'Debater 1' 'Debater 3']\n",
      "Reordered True Relevance: [np.int64(195), np.int64(124), np.int64(124), np.int64(183)]\n",
      "nDCG for Debate 1: 0.888\n",
      "\n",
      "Debate ID: 2\n",
      "True Relevance (original): [121 117 135 186 206]\n",
      "Predicted Order: ['Debater 1' 'Debater 4' 'Debater 5' 'Debater 2' 'Debater 3']\n",
      "Reordered True Relevance: [np.int64(121), np.int64(186), np.int64(206), np.int64(117), np.int64(135)]\n",
      "nDCG for Debate 2: 0.893\n",
      "\n",
      "Debate ID: 3\n",
      "True Relevance (original): [167 140 182 177 145]\n",
      "Predicted Order: ['Debater 4' 'Debater 3' 'Debater 2' 'Debater 1' 'Debater 5']\n",
      "Reordered True Relevance: [np.int64(177), np.int64(182), np.int64(140), np.int64(167), np.int64(145)]\n",
      "nDCG for Debate 3: 0.945\n",
      "\n",
      "Debate ID: 5\n",
      "True Relevance (original): [ 99 104 135 132 157]\n",
      "Predicted Order: ['Debater 3' 'Debater 5' 'Debater 2' 'Debater 1' 'Debater 4']\n",
      "Reordered True Relevance: [np.int64(135), np.int64(157), np.int64(104), np.int64(99), np.int64(132)]\n",
      "nDCG for Debate 5: 0.905\n",
      "\n",
      "Debate ID: 6\n",
      "True Relevance (original): [192 148 168 103]\n",
      "Predicted Order: ['Debater 1' 'Debater 2' 'Debater 3' 'Debater 4']\n",
      "Reordered True Relevance: [np.int64(192), np.int64(148), np.int64(168), np.int64(103)]\n",
      "nDCG for Debate 6: 1.000\n",
      "\n",
      "Debate ID: 7\n",
      "True Relevance (original): [ 91 141 149 129]\n",
      "Predicted Order: ['Debater 2' 'Debater 1' 'Debater 3' 'Debater 4']\n",
      "Reordered True Relevance: [np.int64(141), np.int64(91), np.int64(149), np.int64(129)]\n",
      "nDCG for Debate 7: 0.971\n",
      "\n",
      "Debate ID: 8\n",
      "True Relevance (original): [158 148 132]\n",
      "Predicted Order: ['Debater 1' 'Debater 2' 'Debater 3']\n",
      "Reordered True Relevance: [np.int64(158), np.int64(148), np.int64(132)]\n",
      "nDCG for Debate 8: 1.000\n",
      "\n",
      "Debate ID: 9\n",
      "True Relevance (original): [209 184  84 199]\n",
      "Predicted Order: ['Debater 1' 'Debater 2' 'Debater 3' 'Debater 4']\n",
      "Reordered True Relevance: [np.int64(209), np.int64(184), np.int64(84), np.int64(199)]\n",
      "nDCG for Debate 9: 1.000\n",
      "\n",
      "Debate ID: 11\n",
      "True Relevance (original): [182 168 200 122 202]\n",
      "Predicted Order: ['Debater 3' 'Debater 1' 'Debater 2' 'Debater 4' 'Debater 5']\n",
      "Reordered True Relevance: [np.int64(200), np.int64(182), np.int64(168), np.int64(122), np.int64(202)]\n",
      "nDCG for Debate 11: 0.992\n",
      "\n",
      "Debate ID: 12\n",
      "True Relevance (original): [152 142 204 202]\n",
      "Predicted Order: ['Debater 3' 'Debater 1' 'Debater 2' 'Debater 4']\n",
      "Reordered True Relevance: [np.int64(204), np.int64(152), np.int64(142), np.int64(202)]\n",
      "nDCG for Debate 12: 0.935\n",
      "\n",
      "Debate ID: 13\n",
      "True Relevance (original): [154 187 204]\n",
      "Predicted Order: ['Debater 3' 'Debater 1' 'Debater 2']\n",
      "Reordered True Relevance: [np.int64(204), np.int64(154), np.int64(187)]\n",
      "nDCG for Debate 13: 0.943\n",
      "\n",
      "Debate ID: 14\n",
      "True Relevance (original): [104 182 120 150 169]\n",
      "Predicted Order: ['Debater 2' 'Debater 1' 'Debater 3' 'Debater 4' 'Debater 5']\n",
      "Reordered True Relevance: [np.int64(182), np.int64(104), np.int64(120), np.int64(150), np.int64(169)]\n",
      "nDCG for Debate 14: 0.895\n",
      "\n",
      "Debate ID: 16\n",
      "True Relevance (original): [ 99 120 166 111 118]\n",
      "Predicted Order: ['Debater 3' 'Debater 2' 'Debater 1' 'Debater 4' 'Debater 5']\n",
      "Reordered True Relevance: [np.int64(166), np.int64(120), np.int64(99), np.int64(111), np.int64(118)]\n",
      "nDCG for Debate 16: 0.894\n",
      "\n",
      "Debate ID: 18\n",
      "True Relevance (original): [191 202 193 175]\n",
      "Predicted Order: ['Debater 1' 'Debater 2' 'Debater 3' 'Debater 4']\n",
      "Reordered True Relevance: [np.int64(191), np.int64(202), np.int64(193), np.int64(175)]\n",
      "nDCG for Debate 18: 1.000\n",
      "\n",
      "All nDCG Scores: [np.float64(0.8879199112506109), np.float64(0.8927929943688806), np.float64(0.9451954985675944), np.float64(0.9051885015948088), np.float64(1.0), np.float64(0.9706934530667227), np.float64(0.9999999999999998), np.float64(1.0), np.float64(0.9915077354471094), np.float64(0.9353439982090118), np.float64(0.942919510664788), np.float64(0.8950138839929768), np.float64(0.8937971596527686), np.float64(0.9999999999999998)]\n",
      "Mean nDCG: 0.947\n",
      "\n",
      "Mean nDCG - Hard Voting: 0.735\n",
      "Mean nDCG - Soft Voting: 0.947\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_ndcg(ground_truth_df, predictions_df, k=None, exclude_debates=None):\n",
    "    \"\"\"\n",
    "    Calcula o Mean nDCG para os rankings previstos, utilizando os scores do GT e posições previstas pelos modelos.\n",
    "\n",
    "    Parameters:\n",
    "    - ground_truth_df: DataFrame contendo os dados reais (ground truth).\n",
    "      Deve conter as colunas: 'debate_id', 'position', 'name', 'score'.\n",
    "    - predictions_df: DataFrame contendo as previsões de rankings.\n",
    "      Deve conter as colunas: 'debate_id', 'debater_name', 'debater_position', 'debater_score'.\n",
    "    - k: Inteiro opcional para limitar o cálculo ao top-k. Default: None.\n",
    "    - exclude_debates: Lista de IDs de debates a serem excluídos. Default: None.\n",
    "\n",
    "    Returns:\n",
    "    - mean_ndcg: nDCG médio (float).\n",
    "    \"\"\"\n",
    "    # Filtrar debates, se necessário\n",
    "    if exclude_debates:\n",
    "        ground_truth_df = ground_truth_df[~ground_truth_df['debate_id'].isin(exclude_debates)]\n",
    "        predictions_df = predictions_df[~predictions_df['debate_id'].isin(exclude_debates)]\n",
    "\n",
    "    ndcg_scores = []\n",
    "\n",
    "    for debate_id in ground_truth_df['debate_id'].unique():\n",
    "        ground_truth = ground_truth_df[ground_truth_df['debate_id'] == debate_id]\n",
    "        predictions = predictions_df[predictions_df['debate_id'] == debate_id]\n",
    "\n",
    "        # Garantir consistência\n",
    "        ground_truth = ground_truth.sort_values(by='name').reset_index(drop=True)\n",
    "        predictions = predictions.sort_values(by='debater_name').reset_index(drop=True)\n",
    "\n",
    "        if len(ground_truth) != len(predictions):\n",
    "            raise ValueError(f\"Debate {debate_id} possui tamanhos inconsistentes entre ground truth e previsões.\")\n",
    "\n",
    "        # Relevâncias: usar os scores do GT e ordenar conforme as posições previstas\n",
    "        true_relevance = ground_truth['score'].values\n",
    "        pred_order = predictions.sort_values(by='debater_position')['debater_name'].values\n",
    "\n",
    "        # Reordenar relevâncias do GT com base nas posições previstas\n",
    "        reordered_true_relevance = [true_relevance[list(ground_truth['name']).index(name)] for name in pred_order]\n",
    "\n",
    "        # Debugging: Print values to ensure they are correct\n",
    "        print(f\"Debate ID: {debate_id}\")\n",
    "        print(f\"True Relevance (original): {true_relevance}\")\n",
    "        print(f\"Predicted Order: {pred_order}\")\n",
    "        print(f\"Reordered True Relevance: {reordered_true_relevance}\")\n",
    "\n",
    "        ndcg = ndcg_score([true_relevance], [reordered_true_relevance], k=k)\n",
    "        ndcg_scores.append(ndcg)\n",
    "\n",
    "        print(f\"nDCG for Debate {debate_id}: {ndcg:.3f}\\n\")\n",
    "\n",
    "    mean_ndcg = np.mean(ndcg_scores) if ndcg_scores else 0\n",
    "\n",
    "    # Debugging: Print final results\n",
    "    print(f\"All nDCG Scores: {ndcg_scores}\")\n",
    "    print(f\"Mean nDCG: {mean_ndcg:.3f}\")\n",
    "    return mean_ndcg\n",
    "\n",
    "\n",
    "# Carregar os arquivos CSV\n",
    "hard_voting_df = pd.read_csv('hard_voting.csv')\n",
    "soft_voting_df = pd.read_csv('soft_voting.csv')\n",
    "self_assessment_df = pd.read_csv('rankings_self_assessment_.csv')\n",
    "\n",
    "# Renomear as colunas para uniformizar\n",
    "#hard_voting_df.rename(columns={'position': 'debater_position', 'name': 'debater_name'}, inplace=True)\n",
    "#soft_voting_df.rename(columns={'position': 'debater_position', 'name': 'debater_name'}, inplace=True)\n",
    "\n",
    "# Excluir debates 10 e 17\n",
    "exclude_debates = [10, 17]\n",
    "\n",
    "# Calcular o nDCG para Hard Voting\n",
    "print(\"\\nCalculating nDCG for Hard Voting...\")\n",
    "mean_ndcg_hard = calculate_ndcg(hard_voting_df, self_assessment_df,exclude_debates=exclude_debates)\n",
    "\n",
    "# Calcular o nDCG para Soft Voting\n",
    "print(\"\\nCalculating nDCG for Soft Voting...\")\n",
    "mean_ndcg_soft = calculate_ndcg(soft_voting_df,self_assessment_df, exclude_debates=exclude_debates)\n",
    "\n",
    "# Exibir os resultados\n",
    "print(f\"\\nMean nDCG - Hard Voting: {mean_ndcg_hard:.3f}\")\n",
    "print(f\"Mean nDCG - Soft Voting: {mean_ndcg_soft:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COHENS KAPPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def calculate_cohens_kappa(ground_truth_df, predictions_df, exclude_debates=None):\n",
    "    \"\"\"\n",
    "    Calcula o Cohen's Kappa para as posições dos debatedores.\n",
    "\n",
    "    Parameters:\n",
    "    - ground_truth_df: DataFrame contendo os dados reais (ground truth).\n",
    "      Deve conter as colunas: 'debate_id', 'debater', 'position'.\n",
    "    - predictions_df: DataFrame contendo as previsões de rankings.\n",
    "      Deve conter as colunas: 'debate_id', 'debater', 'position'.\n",
    "    - exclude_debates: Lista de IDs de debates a serem excluídos. Default: None.\n",
    "\n",
    "    Returns:\n",
    "    - kappa_scores: Dicionário com o Cohen's Kappa para cada debate.\n",
    "    - mean_kappa: Média do Cohen's Kappa entre todos os debates.\n",
    "    \"\"\"\n",
    "    # Filtrar debates, se necessário\n",
    "    if exclude_debates:\n",
    "        ground_truth_df = ground_truth_df[~ground_truth_df['debate_id'].isin(exclude_debates)]\n",
    "        predictions_df = predictions_df[~predictions_df['debate_id'].isin(exclude_debates)]\n",
    "\n",
    "    kappa_scores = {}\n",
    "\n",
    "    for debate_id in ground_truth_df['debate_id'].unique():\n",
    "        # Filtrar dados do debate atual\n",
    "        ground_truth = ground_truth_df[ground_truth_df['debate_id'] == debate_id]\n",
    "        predictions = predictions_df[predictions_df['debate_id'] == debate_id]\n",
    "\n",
    "        # Garantir consistência na ordenação por debatedor\n",
    "        ground_truth = ground_truth.sort_values(by='name').reset_index(drop=True)\n",
    "        predictions = predictions.sort_values(by='name').reset_index(drop=True)\n",
    "\n",
    "        if len(ground_truth) != len(predictions):\n",
    "            raise ValueError(f\"Debate {debate_id} possui tamanhos inconsistentes entre ground truth e previsões.\")\n",
    "\n",
    "        # Extração das posições\n",
    "        y_true = ground_truth['position'].values\n",
    "        y_pred = predictions['position'].values\n",
    "\n",
    "        # Calcular o Cohen's Kappa\n",
    "        kappa = cohen_kappa_score(y_true, y_pred)\n",
    "        kappa_scores[debate_id] = kappa\n",
    "\n",
    "    # Calcular a média do Kappa\n",
    "    mean_kappa = sum(kappa_scores.values()) / len(kappa_scores) if kappa_scores else 0\n",
    "\n",
    "    return kappa_scores, mean_kappa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Scores (Hard Voting): {np.int64(1): np.float64(0.19999999999999996), np.int64(2): np.float64(-0.25), np.int64(3): np.float64(0.0), np.int64(5): np.float64(0.2857142857142857), np.int64(6): np.float64(0.5555555555555556), np.int64(7): np.float64(0.33333333333333337), np.int64(8): np.float64(0.5), np.int64(9): np.float64(0.19999999999999996), np.int64(11): np.float64(-0.0714285714285714), np.int64(12): np.float64(0.33333333333333337), np.int64(13): np.float64(0.5), np.int64(14): np.float64(0.25), np.int64(16): np.float64(0.31818181818181823), np.int64(18): np.float64(0.33333333333333337)}\n",
      "Kappa Scores (Soft Voting): {np.int64(1): np.float64(0.19999999999999996), np.int64(2): np.float64(-0.25), np.int64(3): np.float64(0.0), np.int64(5): np.float64(0.0), np.int64(6): np.float64(0.33333333333333337), np.int64(7): np.float64(-0.33333333333333326), np.int64(8): np.float64(0.5), np.int64(9): np.float64(0.0), np.int64(11): np.float64(-0.25), np.int64(12): np.float64(0.33333333333333337), np.int64(13): np.float64(0.5), np.int64(14): np.float64(0.25), np.int64(16): np.float64(0.5), np.int64(18): np.float64(0.33333333333333337)}\n",
      "Mean Cohen's Kappa (Hard Voting): 0.24914450628736345\n",
      "Mean Cohen's Kappa (Soft Voting): 0.15119047619047618\n",
      "\n",
      "Mean Cohen's Kappa - Hard Voting: 0.249\n",
      "Mean Cohen's Kappa - Soft Voting: 0.151\n"
     ]
    }
   ],
   "source": [
    "# Carregar os arquivos CSV\n",
    "hard_voting_df = pd.read_csv('hard_voting.csv')\n",
    "soft_voting_df = pd.read_csv('soft_voting.csv')\n",
    "self_assessment_df = pd.read_csv('rankings_self_assessment_.csv')\n",
    "\n",
    "# Renomear as colunas para uniformizar\n",
    "self_assessment_df.rename(columns={'debater_position': 'position', 'debater_name': 'name'}, inplace=True)\n",
    "\n",
    "# Excluir debates 10 e 17\n",
    "exclude_debates = [10, 17]\n",
    "\n",
    "# Chamada da função para calcular Cohen's Kappa\n",
    "kappa_scores_hard, mean_cohens_hard = calculate_cohens_kappa(hard_voting_df, self_assessment_df, exclude_debates=exclude_debates)\n",
    "kappa_scores_soft, mean_cohens_soft = calculate_cohens_kappa(soft_voting_df, self_assessment_df, exclude_debates=exclude_debates)\n",
    "\n",
    "# Debugging: Verificar o retorno das variáveis\n",
    "print(f\"Kappa Scores (Hard Voting): {kappa_scores_hard}\")\n",
    "print(f\"Kappa Scores (Soft Voting): {kappa_scores_soft}\")\n",
    "print(f\"Mean Cohen's Kappa (Hard Voting): {mean_cohens_hard}\")\n",
    "print(f\"Mean Cohen's Kappa (Soft Voting): {mean_cohens_soft}\")\n",
    "\n",
    "# Exibir os resultados de forma correta\n",
    "print(f\"\\nMean Cohen's Kappa - Hard Voting: {mean_cohens_hard:.3f}\")\n",
    "print(f\"Mean Cohen's Kappa - Soft Voting: {mean_cohens_soft:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
