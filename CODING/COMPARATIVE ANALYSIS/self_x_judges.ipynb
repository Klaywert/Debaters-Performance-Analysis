{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CÓDIGO PARA COMPARAÇÃO SELF ASSESSMENT X AVALIAÇÃO DOS JURADOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACURACIA WINNERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft voting CSV saved to: soft_voting.csv\n",
      "Hard voting CSV saved to: hard_voting.csv\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Placeholder paths for JSON input and CSV outputs\n",
    "input_json_path = \"aggregated_debate_results.json\"\n",
    "soft_voting_csv_path = \"soft_voting.csv\"\n",
    "hard_voting_csv_path = \"hard_voting.csv\"\n",
    "\n",
    "# Load JSON data\n",
    "with open(input_json_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Prepare lists for each CSV\n",
    "soft_voting_rows = []\n",
    "hard_voting_rows = []\n",
    "\n",
    "# Process the JSON data to extract rows for both CSVs\n",
    "for debate in data:\n",
    "    debate_id = debate[\"debate_id\"]\n",
    "    for entry in debate[\"soft_voting_ranking\"]:\n",
    "        soft_voting_rows.append(\n",
    "            {\"debate_id\": debate_id, \"position\": entry[\"position\"], \"name\": entry[\"name\"], \"score\": entry[\"score\"]}\n",
    "        )\n",
    "    for entry in debate[\"hard_voting_ranking\"]:\n",
    "        hard_voting_rows.append(\n",
    "            {\"debate_id\": debate_id, \"position\": entry[\"position\"], \"name\": entry[\"name\"], \"score\": entry[\"score\"]}\n",
    "        )\n",
    "\n",
    "# Create DataFrames\n",
    "soft_voting_df = pd.DataFrame(soft_voting_rows)\n",
    "hard_voting_df = pd.DataFrame(hard_voting_rows)\n",
    "\n",
    "# Save to CSV files\n",
    "soft_voting_df.to_csv(soft_voting_csv_path, index=False)\n",
    "hard_voting_df.to_csv(hard_voting_csv_path, index=False)\n",
    "\n",
    "print(f\"Soft voting CSV saved to: {soft_voting_csv_path}\")\n",
    "print(f\"Hard voting CSV saved to: {hard_voting_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Hard Voting:\n",
      "Accuracy of winners (hard voting): 71.43%\n",
      "Debate ID: 1\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 2\n",
      "Expected winners: Debater 1, Debater 4\n",
      "Predicted winners: Debater 5\n",
      "----------------------------------------\n",
      "Debate ID: 3\n",
      "Expected winners: Debater 4\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 5\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 5\n",
      "----------------------------------------\n",
      "Debate ID: 6\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 7\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 8\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 9\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 1, Debater 4\n",
      "----------------------------------------\n",
      "Debate ID: 11\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 5\n",
      "----------------------------------------\n",
      "Debate ID: 12\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 13\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 14\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 16\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 18\n",
      "Expected winners: Debater 3, Debater 1, Debater 2\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "\n",
      "Evaluating Soft Voting:\n",
      "Accuracy of winners (soft voting): 64.29%\n",
      "Debate ID: 1\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 2\n",
      "Expected winners: Debater 1, Debater 4\n",
      "Predicted winners: Debater 5\n",
      "----------------------------------------\n",
      "Debate ID: 3\n",
      "Expected winners: Debater 4\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 5\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 5\n",
      "----------------------------------------\n",
      "Debate ID: 6\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 7\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 8\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 9\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 11\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 5\n",
      "----------------------------------------\n",
      "Debate ID: 12\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 13\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 14\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 16\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 18\n",
      "Expected winners: Debater 3, Debater 1, Debater 2\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def evaluate_predictions(ground_truth_path, predictions_path, excluded_debates, voting_type):\n",
    "    \"\"\"\n",
    "    Avalia a precisão das predições com base nos vencedores previstos e reais.\n",
    "    \n",
    "    Args:\n",
    "        ground_truth_path (str): Caminho para o arquivo CSV de ground truth (self-assessment).\n",
    "        predictions_path (str): Caminho para o arquivo CSV das predições (hard ou soft voting).\n",
    "        excluded_debates (list): Lista de IDs de debates a serem excluídos.\n",
    "        voting_type (str): Tipo de votação ('hard' ou 'soft') para indicar o tipo de predictions.\n",
    "    \n",
    "    Returns:\n",
    "        None: Exibe a precisão calculada e detalhes.\n",
    "    \"\"\"\n",
    "    # Load datasets\n",
    "    ground_truth_df = pd.read_csv(ground_truth_path)\n",
    "    predictions_df = pd.read_csv(predictions_path)\n",
    "\n",
    "    # Exclude specific debates\n",
    "    ground_truth_df = ground_truth_df[~ground_truth_df['debate_id'].isin(excluded_debates)]\n",
    "    predictions_df = predictions_df[~predictions_df['debate_id'].isin(excluded_debates)]\n",
    "\n",
    "    # Rename columns to ensure consistency\n",
    "    ground_truth_df = ground_truth_df.rename(columns={\n",
    "        'debater_name': 'name',\n",
    "        'debater_position': 'position'\n",
    "    })\n",
    "\n",
    "    # Merge datasets on debate_id\n",
    "    merged_df = pd.merge(\n",
    "        ground_truth_df[['debate_id', 'name', 'position']],\n",
    "        predictions_df[['debate_id', 'name', 'position']],\n",
    "        on='debate_id',\n",
    "        suffixes=('_true', '_pred')\n",
    "    )\n",
    "\n",
    "    # Identify ground truth winners\n",
    "    ground_truth_winners = merged_df[merged_df['position_true'] == 1]\n",
    "\n",
    "    # Initialize list for accuracy calculations and details\n",
    "    correct_predictions = []\n",
    "    debate_details = []\n",
    "\n",
    "    for debate_id, group in ground_truth_winners.groupby('debate_id'):\n",
    "        # Get the names of the ground truth winners\n",
    "        true_winners = set(group['name_true'])\n",
    "        \n",
    "        # Get the predicted winners for this debate_id\n",
    "        predicted_winners = set(\n",
    "            merged_df[\n",
    "                (merged_df['debate_id'] == debate_id) & \n",
    "                (merged_df['position_pred'] == 1)\n",
    "            ]['name_pred']\n",
    "        )\n",
    "        \n",
    "        # Save the details for display\n",
    "        debate_details.append({\n",
    "            'debate_id': debate_id,\n",
    "            'true_winners': true_winners,\n",
    "            'predicted_winners': predicted_winners\n",
    "        })\n",
    "        \n",
    "        # If there is an intersection between true and predicted winners, it's a correct prediction\n",
    "        if true_winners.intersection(predicted_winners):\n",
    "            correct_predictions.append(1)\n",
    "        else:\n",
    "            correct_predictions.append(0)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = sum(correct_predictions) / len(correct_predictions)\n",
    "    print(f'Accuracy of winners ({voting_type} voting): {accuracy * 100:.2f}%')\n",
    "\n",
    "    # Print the expected and predicted winners for each debate\n",
    "    for detail in debate_details:\n",
    "        print(f\"Debate ID: {detail['debate_id']}\")\n",
    "        print(f\"Expected winners: {', '.join(detail['true_winners'])}\")\n",
    "        print(f\"Predicted winners: {', '.join(detail['predicted_winners'])}\")\n",
    "        print('-' * 40)\n",
    "\n",
    "# File paths\n",
    "hard_voting_path = 'hard_voting.csv'\n",
    "soft_voting_path = 'soft_voting.csv'\n",
    "self_assessment_path = 'rankings_self_assessment_.csv'\n",
    "\n",
    "# Excluded debates\n",
    "excluded_debates = [10, 17]\n",
    "\n",
    "# Evaluate for hard voting\n",
    "print(\"Evaluating Hard Voting:\")\n",
    "evaluate_predictions(self_assessment_path, hard_voting_path, excluded_debates, 'hard')\n",
    "\n",
    "# Evaluate for soft voting\n",
    "print(\"\\nEvaluating Soft Voting:\")\n",
    "evaluate_predictions(self_assessment_path, soft_voting_path, excluded_debates, 'soft')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACURACIA DEBATERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Avaliando Hard Voting ---\n",
      "\n",
      "Debate ID: 1\n",
      "Ranking esperado: Debater 2, Debater 3, Debater 1, Debater 4\n",
      "Ranking previsto: Debater 2, Debater 4, Debater 1, Debater 3\n",
      "2 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 2\n",
      "Ranking esperado: Debater 5, Debater 4, Debater 1, Debater 2, Debater 3\n",
      "Ranking previsto: Debater 1, Debater 4, Debater 5, Debater 2, Debater 3\n",
      "3 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 3\n",
      "Ranking esperado: Debater 3, Debater 4, Debater 1, Debater 2, Debater 5\n",
      "Ranking previsto: Debater 4, Debater 3, Debater 1, Debater 2, Debater 5\n",
      "3 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 5\n",
      "Ranking esperado: Debater 5, Debater 3, Debater 1, Debater 2, Debater 4\n",
      "Ranking previsto: Debater 3, Debater 5, Debater 1, Debater 2, Debater 4\n",
      "3 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 6\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "4 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 7\n",
      "Ranking esperado: Debater 2, Debater 3, Debater 1, Debater 4\n",
      "Ranking previsto: Debater 2, Debater 1, Debater 3, Debater 4\n",
      "2 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 8\n",
      "Ranking esperado: Debater 1, Debater 3, Debater 2\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3\n",
      "1 acertos de 3 debatedores.\n",
      "\n",
      "Debate ID: 9\n",
      "Ranking esperado: Debater 1, Debater 4, Debater 2, Debater 3\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "1 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 11\n",
      "Ranking esperado: Debater 5, Debater 3, Debater 1, Debater 2, Debater 4\n",
      "Ranking previsto: Debater 3, Debater 1, Debater 2, Debater 4, Debater 5\n",
      "0 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 12\n",
      "Ranking esperado: Debater 3, Debater 4, Debater 1, Debater 2\n",
      "Ranking previsto: Debater 3, Debater 1, Debater 2, Debater 4\n",
      "1 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 13\n",
      "Ranking esperado: Debater 3, Debater 2, Debater 1\n",
      "Ranking previsto: Debater 3, Debater 1, Debater 2\n",
      "1 acertos de 3 debatedores.\n",
      "\n",
      "Debate ID: 14\n",
      "Ranking esperado: Debater 2, Debater 5, Debater 1, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 2, Debater 1, Debater 3, Debater 4, Debater 5\n",
      "1 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 16\n",
      "Ranking esperado: Debater 3, Debater 2, Debater 4, Debater 1, Debater 5\n",
      "Ranking previsto: Debater 3, Debater 2, Debater 1, Debater 4, Debater 5\n",
      "3 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 18\n",
      "Ranking esperado: Debater 3, Debater 1, Debater 2, Debater 4\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "1 acertos de 4 debatedores.\n",
      "\n",
      "Acurácia de ranking: 42.98%\n",
      "Total de acertos: 26 de 60 debatedores (43.33%)\n",
      "\n",
      "--- Avaliando Soft Voting ---\n",
      "\n",
      "Debate ID: 1\n",
      "Ranking esperado: Debater 2, Debater 3, Debater 1, Debater 4\n",
      "Ranking previsto: Debater 2, Debater 4, Debater 1, Debater 3\n",
      "2 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 2\n",
      "Ranking esperado: Debater 5, Debater 4, Debater 3, Debater 1, Debater 2\n",
      "Ranking previsto: Debater 1, Debater 4, Debater 5, Debater 2, Debater 3\n",
      "1 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 3\n",
      "Ranking esperado: Debater 3, Debater 4, Debater 1, Debater 5, Debater 2\n",
      "Ranking previsto: Debater 4, Debater 3, Debater 1, Debater 2, Debater 5\n",
      "1 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 5\n",
      "Ranking esperado: Debater 5, Debater 3, Debater 4, Debater 2, Debater 1\n",
      "Ranking previsto: Debater 3, Debater 5, Debater 1, Debater 2, Debater 4\n",
      "1 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 6\n",
      "Ranking esperado: Debater 1, Debater 3, Debater 2, Debater 4\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "2 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 7\n",
      "Ranking esperado: Debater 3, Debater 2, Debater 4, Debater 1\n",
      "Ranking previsto: Debater 2, Debater 1, Debater 3, Debater 4\n",
      "0 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 8\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3\n",
      "3 acertos de 3 debatedores.\n",
      "\n",
      "Debate ID: 9\n",
      "Ranking esperado: Debater 1, Debater 4, Debater 2, Debater 3\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "1 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 11\n",
      "Ranking esperado: Debater 5, Debater 3, Debater 1, Debater 2, Debater 4\n",
      "Ranking previsto: Debater 3, Debater 1, Debater 2, Debater 4, Debater 5\n",
      "0 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 12\n",
      "Ranking esperado: Debater 3, Debater 4, Debater 1, Debater 2\n",
      "Ranking previsto: Debater 3, Debater 1, Debater 2, Debater 4\n",
      "1 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 13\n",
      "Ranking esperado: Debater 3, Debater 2, Debater 1\n",
      "Ranking previsto: Debater 3, Debater 1, Debater 2\n",
      "1 acertos de 3 debatedores.\n",
      "\n",
      "Debate ID: 14\n",
      "Ranking esperado: Debater 2, Debater 5, Debater 4, Debater 3, Debater 1\n",
      "Ranking previsto: Debater 2, Debater 1, Debater 3, Debater 4, Debater 5\n",
      "1 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 16\n",
      "Ranking esperado: Debater 3, Debater 2, Debater 5, Debater 4, Debater 1\n",
      "Ranking previsto: Debater 3, Debater 2, Debater 1, Debater 4, Debater 5\n",
      "3 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 18\n",
      "Ranking esperado: Debater 2, Debater 3, Debater 1, Debater 4\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "1 acertos de 4 debatedores.\n",
      "\n",
      "Acurácia de ranking: 32.02%\n",
      "Total de acertos: 18 de 60 debatedores (30.00%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_ranking_accuracy(ground_truth_df, predictions_df, excluded_debates=[10, 17]):\n",
    "    \"\"\"\n",
    "    Calcula a acurácia do ranking comparando as previsões de rankings com os rankings reais (ground truth).\n",
    "\n",
    "    Parameters:\n",
    "    - ground_truth_df: DataFrame com os rankings reais (hard ou soft voting).\n",
    "    - predictions_df: DataFrame com as previsões dos modelos (ex: self-assessment).\n",
    "    - excluded_debates: Lista de debates a serem excluídos da análise (default: [10, 17]).\n",
    "\n",
    "    Returns:\n",
    "    - ranking_accuracy: Acurácia do ranking (em porcentagem).\n",
    "    \"\"\"\n",
    "\n",
    "    # Filtra os debates a serem excluídos\n",
    "    ground_truth_df = ground_truth_df[~ground_truth_df['debate_id'].isin(excluded_debates)]\n",
    "    predictions_df = predictions_df[~predictions_df['debate_id'].isin(excluded_debates)]\n",
    "\n",
    "    # Lista para armazenar as acurácias de ranking de cada debate\n",
    "    correct_rankings = []\n",
    "    debate_details = []\n",
    "\n",
    "    # Variáveis para acumular o número total de acertos e debatedores\n",
    "    total_correct = 0\n",
    "    total_debaters = 0\n",
    "\n",
    "    # Para cada debate, compara o ranking completo dos debatedores\n",
    "    for debate_id in ground_truth_df['debate_id'].unique():\n",
    "        # Filtra os debatedores do debate real e do modelo\n",
    "        true_ranking = ground_truth_df[ground_truth_df['debate_id'] == debate_id][['name', 'position']]\n",
    "        predicted_ranking = predictions_df[predictions_df['debate_id'] == debate_id][['debater_name', 'debater_position']]\n",
    "\n",
    "        # Ordena os debatedores conforme suas posições no ranking (ground truth e previsão)\n",
    "        true_ranking_sorted = true_ranking.sort_values(by='position').reset_index(drop=True)\n",
    "        predicted_ranking_sorted = predicted_ranking.sort_values(by='debater_position').reset_index(drop=True)\n",
    "\n",
    "        # Salva os detalhes do debate para exibição\n",
    "        debate_details.append({\n",
    "            'debate_id': debate_id,\n",
    "            'true_ranking': true_ranking_sorted['name'].tolist(),\n",
    "            'predicted_ranking': predicted_ranking_sorted['debater_name'].tolist()\n",
    "        })\n",
    "\n",
    "        # Inicializa o contador de acertos considerando empates\n",
    "        correct_rankings_count = 0\n",
    "\n",
    "        # Compara os rankings completos (considerando a ordem exata de posições)\n",
    "        for i in range(len(true_ranking_sorted)):\n",
    "            if true_ranking_sorted.iloc[i]['name'] == predicted_ranking_sorted.iloc[i]['debater_name']:\n",
    "                correct_rankings_count += 1\n",
    "\n",
    "        # Calcula a fração de debatedores classificados corretamente neste debate\n",
    "        correct_rankings.append(correct_rankings_count / len(true_ranking_sorted))\n",
    "\n",
    "        # Acumula os valores totais\n",
    "        total_correct += correct_rankings_count\n",
    "        total_debaters += len(true_ranking_sorted)\n",
    "\n",
    "        # Exibe informações do debate\n",
    "        print(f\"Debate ID: {debate_id}\")\n",
    "        print(f\"Ranking esperado: {', '.join(true_ranking_sorted['name'])}\")\n",
    "        print(f\"Ranking previsto: {', '.join(predicted_ranking_sorted['debater_name'])}\")\n",
    "        print(f\"{correct_rankings_count} acertos de {len(true_ranking_sorted)} debatedores.\\n\")\n",
    "\n",
    "    # Calcula a acurácia média de ranking\n",
    "    ranking_accuracy = sum(correct_rankings) / len(correct_rankings) if correct_rankings else 0\n",
    "\n",
    "    # Exibe a acurácia do ranking\n",
    "    print(f'Acurácia de ranking: {ranking_accuracy * 100:.2f}%')\n",
    "\n",
    "    # Exibe o número total de acertos dividido pelo número total de debatedores\n",
    "    print(f\"Total de acertos: {total_correct} de {total_debaters} debatedores ({(total_correct / total_debaters) * 100:.2f}%)\")\n",
    "\n",
    "    return ranking_accuracy\n",
    "\n",
    "# Função principal para avaliar hard e soft voting\n",
    "def evaluate_voting(ground_truth_path, predictions_path, excluded_debates, voting_type):\n",
    "    print(f\"\\n--- Avaliando {voting_type.capitalize()} Voting ---\\n\")\n",
    "\n",
    "    # Carregar os datasets\n",
    "    ground_truth_df = pd.read_csv(ground_truth_path)\n",
    "    predictions_df = pd.read_csv(predictions_path)\n",
    "\n",
    "    # Ajustar nomes das colunas para uniformizar\n",
    "    ground_truth_df.rename(columns={'position': 'position', 'name': 'name'}, inplace=True)\n",
    "    predictions_df.rename(columns={'debater_position': 'debater_position', 'debater_name': 'debater_name'}, inplace=True)\n",
    "\n",
    "    # Calcular acurácia do ranking\n",
    "    calculate_ranking_accuracy(ground_truth_df, predictions_df, excluded_debates)\n",
    "\n",
    "# Caminhos para os arquivos\n",
    "hard_voting_path = \"hard_voting.csv\"\n",
    "soft_voting_path = \"soft_voting.csv\"\n",
    "self_assessment_path = \"rankings_self_assessment_.csv\"\n",
    "\n",
    "# Excluir debates\n",
    "excluded_debates = [10, 17]\n",
    "\n",
    "# Avaliar para hard voting\n",
    "evaluate_voting(hard_voting_path, self_assessment_path, excluded_debates, 'hard')\n",
    "\n",
    "# Avaliar para soft voting\n",
    "evaluate_voting(soft_voting_path, self_assessment_path, excluded_debates, 'soft')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_mrr(hard_voting_df, soft_voting_df, self_assessment_df, excluded_debates=[10, 17]):\n",
    "    \"\"\"\n",
    "    Calcula o Mean Reciprocal Rank (MRR) comparando as previsões de rankings com os rankings reais (ground truth).\n",
    "    \n",
    "    Parameters:\n",
    "    - hard_voting_df: DataFrame com as previsões dos votos duros (hard voting).\n",
    "    - soft_voting_df: DataFrame com as previsões dos votos suaves (soft voting).\n",
    "    - self_assessment_df: DataFrame com as autoavaliações (ground truth).\n",
    "    - excluded_debates: Lista de debates a serem excluídos da análise (default: [10, 17]).\n",
    "    \n",
    "    Returns:\n",
    "    - mrr_hard: Mean Reciprocal Rank (em decimal) para hard voting.\n",
    "    - mrr_soft: Mean Reciprocal Rank (em decimal) para soft voting.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filtra os debates a serem excluídos\n",
    "    hard_voting_df = hard_voting_df[~hard_voting_df['debate_id'].isin(excluded_debates)]\n",
    "    soft_voting_df = soft_voting_df[~soft_voting_df['debate_id'].isin(excluded_debates)]\n",
    "    self_assessment_df = self_assessment_df[~self_assessment_df['debate_id'].isin(excluded_debates)]\n",
    "\n",
    "    # Lista para armazenar os Reciprocal Ranks de cada debate\n",
    "    reciprocal_ranks_hard = []\n",
    "    reciprocal_ranks_soft = []\n",
    "\n",
    "    # Para cada debate, calcula o Reciprocal Rank para hard e soft voting\n",
    "    for debate_id in self_assessment_df['debate_id'].unique():\n",
    "        # Filtra os debatedores do debate real (ground truth)\n",
    "        true_ranking = self_assessment_df[self_assessment_df['debate_id'] == debate_id][['debater_name', 'debater_position']]\n",
    "        \n",
    "        # Filtra os rankings de predições (hard e soft voting)\n",
    "        predicted_ranking_hard = hard_voting_df[hard_voting_df['debate_id'] == debate_id][['name', 'position']]\n",
    "        predicted_ranking_soft = soft_voting_df[soft_voting_df['debate_id'] == debate_id][['name', 'position']]\n",
    "\n",
    "        # Ordena os debatedores conforme suas posições no ranking (ground truth e previsões)\n",
    "        true_ranking_sorted = true_ranking.sort_values(by='debater_position').reset_index(drop=True)\n",
    "        predicted_ranking_hard_sorted = predicted_ranking_hard.sort_values(by='position').reset_index(drop=True)\n",
    "        predicted_ranking_soft_sorted = predicted_ranking_soft.sort_values(by='position').reset_index(drop=True)\n",
    "\n",
    "        # Lista de vencedores no ground truth (pode haver empate)\n",
    "        true_winners = set(true_ranking_sorted[true_ranking_sorted['debater_position'] == 1]['debater_name'])\n",
    "\n",
    "        # Calcula o Reciprocal Rank para hard voting\n",
    "        rr_hard = 0\n",
    "        for rank, debater in enumerate(predicted_ranking_hard_sorted['name'], start=1):\n",
    "            if debater in true_winners:\n",
    "                rr_hard = 1 / rank\n",
    "                break\n",
    "        \n",
    "        # Calcula o Reciprocal Rank para soft voting\n",
    "        rr_soft = 0\n",
    "        for rank, debater in enumerate(predicted_ranking_soft_sorted['name'], start=1):\n",
    "            if debater in true_winners:\n",
    "                rr_soft = 1 / rank\n",
    "                break\n",
    "        \n",
    "        # Armazena os RRs dos debates\n",
    "        reciprocal_ranks_hard.append(rr_hard)\n",
    "        reciprocal_ranks_soft.append(rr_soft)\n",
    "\n",
    "        # Exibe informações do debate\n",
    "        print(f\"Debate ID: {debate_id}\")\n",
    "        print(f\"Vencedores esperados: {', '.join(true_winners)}\")\n",
    "        print(f\"Ranking previsto (Hard Voting): {', '.join(predicted_ranking_hard_sorted['name'])}\")\n",
    "        print(f\"Ranking previsto (Soft Voting): {', '.join(predicted_ranking_soft_sorted['name'])}\")\n",
    "        print(f\"Reciprocal Rank (Hard Voting): {rr_hard:.3f}\")\n",
    "        print(f\"Reciprocal Rank (Soft Voting): {rr_soft:.3f}\\n\")\n",
    "\n",
    "    # Calcula o MRR para hard e soft voting\n",
    "    mrr_hard = sum(reciprocal_ranks_hard) / len(reciprocal_ranks_hard) if reciprocal_ranks_hard else 0\n",
    "    mrr_soft = sum(reciprocal_ranks_soft) / len(reciprocal_ranks_soft) if reciprocal_ranks_soft else 0\n",
    "    \n",
    "    # Exibe o MRR\n",
    "    print(f\"Mean Reciprocal Rank (MRR) - Hard Voting: {mrr_hard:.3f}\")\n",
    "    print(f\"Mean Reciprocal Rank (MRR) - Soft Voting: {mrr_soft:.3f}\")\n",
    "\n",
    "    return mrr_hard, mrr_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debate ID: 1\n",
      "Vencedores esperados: Debater 2\n",
      "Ranking previsto (Hard Voting): Debater 2, Debater 3, Debater 1, Debater 4\n",
      "Ranking previsto (Soft Voting): Debater 2, Debater 3, Debater 1, Debater 4\n",
      "Reciprocal Rank (Hard Voting): 1.000\n",
      "Reciprocal Rank (Soft Voting): 1.000\n",
      "\n",
      "Debate ID: 2\n",
      "Vencedores esperados: Debater 1, Debater 4\n",
      "Ranking previsto (Hard Voting): Debater 5, Debater 4, Debater 1, Debater 2, Debater 3\n",
      "Ranking previsto (Soft Voting): Debater 5, Debater 4, Debater 3, Debater 1, Debater 2\n",
      "Reciprocal Rank (Hard Voting): 0.500\n",
      "Reciprocal Rank (Soft Voting): 0.500\n",
      "\n",
      "Debate ID: 3\n",
      "Vencedores esperados: Debater 4\n",
      "Ranking previsto (Hard Voting): Debater 3, Debater 4, Debater 1, Debater 2, Debater 5\n",
      "Ranking previsto (Soft Voting): Debater 3, Debater 4, Debater 1, Debater 5, Debater 2\n",
      "Reciprocal Rank (Hard Voting): 0.500\n",
      "Reciprocal Rank (Soft Voting): 0.500\n",
      "\n",
      "Debate ID: 5\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto (Hard Voting): Debater 5, Debater 3, Debater 1, Debater 2, Debater 4\n",
      "Ranking previsto (Soft Voting): Debater 5, Debater 3, Debater 4, Debater 2, Debater 1\n",
      "Reciprocal Rank (Hard Voting): 0.500\n",
      "Reciprocal Rank (Soft Voting): 0.500\n",
      "\n",
      "Debate ID: 6\n",
      "Vencedores esperados: Debater 1\n",
      "Ranking previsto (Hard Voting): Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Ranking previsto (Soft Voting): Debater 1, Debater 3, Debater 2, Debater 4\n",
      "Reciprocal Rank (Hard Voting): 1.000\n",
      "Reciprocal Rank (Soft Voting): 1.000\n",
      "\n",
      "Debate ID: 7\n",
      "Vencedores esperados: Debater 2\n",
      "Ranking previsto (Hard Voting): Debater 2, Debater 3, Debater 1, Debater 4\n",
      "Ranking previsto (Soft Voting): Debater 3, Debater 2, Debater 4, Debater 1\n",
      "Reciprocal Rank (Hard Voting): 1.000\n",
      "Reciprocal Rank (Soft Voting): 0.500\n",
      "\n",
      "Debate ID: 8\n",
      "Vencedores esperados: Debater 1\n",
      "Ranking previsto (Hard Voting): Debater 1, Debater 3, Debater 2\n",
      "Ranking previsto (Soft Voting): Debater 1, Debater 2, Debater 3\n",
      "Reciprocal Rank (Hard Voting): 1.000\n",
      "Reciprocal Rank (Soft Voting): 1.000\n",
      "\n",
      "Debate ID: 9\n",
      "Vencedores esperados: Debater 1\n",
      "Ranking previsto (Hard Voting): Debater 1, Debater 4, Debater 2, Debater 3\n",
      "Ranking previsto (Soft Voting): Debater 1, Debater 4, Debater 2, Debater 3\n",
      "Reciprocal Rank (Hard Voting): 1.000\n",
      "Reciprocal Rank (Soft Voting): 1.000\n",
      "\n",
      "Debate ID: 11\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto (Hard Voting): Debater 5, Debater 3, Debater 1, Debater 2, Debater 4\n",
      "Ranking previsto (Soft Voting): Debater 5, Debater 3, Debater 1, Debater 2, Debater 4\n",
      "Reciprocal Rank (Hard Voting): 0.500\n",
      "Reciprocal Rank (Soft Voting): 0.500\n",
      "\n",
      "Debate ID: 12\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto (Hard Voting): Debater 3, Debater 4, Debater 1, Debater 2\n",
      "Ranking previsto (Soft Voting): Debater 3, Debater 4, Debater 1, Debater 2\n",
      "Reciprocal Rank (Hard Voting): 1.000\n",
      "Reciprocal Rank (Soft Voting): 1.000\n",
      "\n",
      "Debate ID: 13\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto (Hard Voting): Debater 3, Debater 2, Debater 1\n",
      "Ranking previsto (Soft Voting): Debater 3, Debater 2, Debater 1\n",
      "Reciprocal Rank (Hard Voting): 1.000\n",
      "Reciprocal Rank (Soft Voting): 1.000\n",
      "\n",
      "Debate ID: 14\n",
      "Vencedores esperados: Debater 2\n",
      "Ranking previsto (Hard Voting): Debater 2, Debater 5, Debater 1, Debater 3, Debater 4\n",
      "Ranking previsto (Soft Voting): Debater 2, Debater 5, Debater 4, Debater 3, Debater 1\n",
      "Reciprocal Rank (Hard Voting): 1.000\n",
      "Reciprocal Rank (Soft Voting): 1.000\n",
      "\n",
      "Debate ID: 16\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto (Hard Voting): Debater 3, Debater 2, Debater 4, Debater 1, Debater 5\n",
      "Ranking previsto (Soft Voting): Debater 3, Debater 2, Debater 5, Debater 4, Debater 1\n",
      "Reciprocal Rank (Hard Voting): 1.000\n",
      "Reciprocal Rank (Soft Voting): 1.000\n",
      "\n",
      "Debate ID: 18\n",
      "Vencedores esperados: Debater 3, Debater 1, Debater 2\n",
      "Ranking previsto (Hard Voting): Debater 3, Debater 1, Debater 2, Debater 4\n",
      "Ranking previsto (Soft Voting): Debater 2, Debater 3, Debater 1, Debater 4\n",
      "Reciprocal Rank (Hard Voting): 1.000\n",
      "Reciprocal Rank (Soft Voting): 1.000\n",
      "\n",
      "Mean Reciprocal Rank (MRR) - Hard Voting: 0.857\n",
      "Mean Reciprocal Rank (MRR) - Soft Voting: 0.821\n",
      "MRR - Hard Voting: 0.857\n",
      "MRR - Soft Voting: 0.821\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carregar os arquivos CSV\n",
    "self_assessment_df = pd.read_csv('rankings_self_assessment_.csv')\n",
    "hard_voting_df = pd.read_csv('hard_voting.csv')\n",
    "soft_voting_df = pd.read_csv('soft_voting.csv')\n",
    "\n",
    "# Chamar a função para calcular o MRR\n",
    "mrr_hard, mrr_soft = calculate_mrr(hard_voting_df, soft_voting_df, self_assessment_df)\n",
    "\n",
    "# Exibir os resultados\n",
    "print(f\"MRR - Hard Voting: {mrr_hard:.3f}\")\n",
    "print(f\"MRR - Soft Voting: {mrr_soft:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating nDCG for Hard Voting...\n",
      "\n",
      "Processing Debate ID: 1\n",
      "Ground truth scores: [0. 2. 0. 1.]\n",
      "Predicted scores: [0 3 2 0]\n",
      "nDCG for this debate: 0.937059712708037\n",
      "\n",
      "Processing Debate ID: 2\n",
      "Ground truth scores: [2. 0. 0. 2. 1.]\n",
      "Predicted scores: [0 0 0 2 5]\n",
      "nDCG for this debate: 0.8347500515066167\n",
      "\n",
      "Processing Debate ID: 3\n",
      "Ground truth scores: [0. 0. 1. 4. 0.]\n",
      "Predicted scores: [1 0 3 2 0]\n",
      "nDCG for this debate: 0.760909623292876\n",
      "\n",
      "Processing Debate ID: 5\n",
      "Ground truth scores: [0. 0. 2. 0. 1.]\n",
      "Predicted scores: [0 0 2 0 4]\n",
      "nDCG for this debate: 0.8597186998521971\n",
      "\n",
      "Processing Debate ID: 6\n",
      "Ground truth scores: [3. 0. 0. 0.]\n",
      "Predicted scores: [4 2 2 0]\n",
      "nDCG for this debate: 1.0\n",
      "\n",
      "Processing Debate ID: 7\n",
      "Ground truth scores: [0. 3. 0. 0.]\n",
      "Predicted scores: [1 4 2 1]\n",
      "nDCG for this debate: 1.0\n",
      "\n",
      "Processing Debate ID: 8\n",
      "Ground truth scores: [1. 0. 0.]\n",
      "Predicted scores: [5 0 1]\n",
      "nDCG for this debate: 1.0\n",
      "\n",
      "Processing Debate ID: 9\n",
      "Ground truth scores: [2. 1. 0. 0.]\n",
      "Predicted scores: [3 0 0 3]\n",
      "nDCG for this debate: 0.7967784125602342\n",
      "\n",
      "Processing Debate ID: 11\n",
      "Ground truth scores: [1. 0. 2. 0. 0.]\n",
      "Predicted scores: [0 0 2 0 5]\n",
      "nDCG for this debate: 0.6465531662091785\n",
      "\n",
      "Processing Debate ID: 12\n",
      "Ground truth scores: [0. 0. 3. 0.]\n",
      "Predicted scores: [0 0 4 2]\n",
      "nDCG for this debate: 1.0\n",
      "\n",
      "Processing Debate ID: 13\n",
      "Ground truth scores: [0. 0. 1.]\n",
      "Predicted scores: [0 3 4]\n",
      "nDCG for this debate: 1.0\n",
      "\n",
      "Processing Debate ID: 14\n",
      "Ground truth scores: [0. 3. 0. 0. 0.]\n",
      "Predicted scores: [0 4 0 0 3]\n",
      "nDCG for this debate: 1.0\n",
      "\n",
      "Processing Debate ID: 16\n",
      "Ground truth scores: [0. 1. 4. 0. 0.]\n",
      "Predicted scores: [0 1 3 1 0]\n",
      "nDCG for this debate: 0.9858635565060685\n",
      "\n",
      "Processing Debate ID: 18\n",
      "Ground truth scores: [1. 1. 1. 0.]\n",
      "Predicted scores: [2 2 3 1]\n",
      "nDCG for this debate: 1.0\n",
      "\n",
      "Mean nDCG across all debates: 0.9158309444739434\n",
      "\n",
      "Calculating nDCG for Soft Voting...\n",
      "\n",
      "Processing Debate ID: 1\n",
      "Ground truth scores: [0. 2. 0. 1.]\n",
      "Predicted scores: [124 195 183 124]\n",
      "nDCG for this debate: 0.937059712708037\n",
      "\n",
      "Processing Debate ID: 2\n",
      "Ground truth scores: [2. 0. 0. 2. 1.]\n",
      "Predicted scores: [121 117 135 186 206]\n",
      "nDCG for this debate: 0.8302310645465177\n",
      "\n",
      "Processing Debate ID: 3\n",
      "Ground truth scores: [0. 0. 1. 4. 0.]\n",
      "Predicted scores: [167 140 182 177 145]\n",
      "nDCG for this debate: 0.760909623292876\n",
      "\n",
      "Processing Debate ID: 5\n",
      "Ground truth scores: [0. 0. 2. 0. 1.]\n",
      "Predicted scores: [ 99 104 135 132 157]\n",
      "nDCG for this debate: 0.8597186998521971\n",
      "\n",
      "Processing Debate ID: 6\n",
      "Ground truth scores: [3. 0. 0. 0.]\n",
      "Predicted scores: [192 148 168 103]\n",
      "nDCG for this debate: 1.0\n",
      "\n",
      "Processing Debate ID: 7\n",
      "Ground truth scores: [0. 3. 0. 0.]\n",
      "Predicted scores: [ 91 141 149 129]\n",
      "nDCG for this debate: 0.6309297535714573\n",
      "\n",
      "Processing Debate ID: 8\n",
      "Ground truth scores: [1. 0. 0.]\n",
      "Predicted scores: [158 148 132]\n",
      "nDCG for this debate: 1.0\n",
      "\n",
      "Processing Debate ID: 9\n",
      "Ground truth scores: [2. 1. 0. 0.]\n",
      "Predicted scores: [209 184  84 199]\n",
      "nDCG for this debate: 0.9502344167898358\n",
      "\n",
      "Processing Debate ID: 11\n",
      "Ground truth scores: [1. 0. 2. 0. 0.]\n",
      "Predicted scores: [182 168 200 122 202]\n",
      "nDCG for this debate: 0.6696718164942299\n",
      "\n",
      "Processing Debate ID: 12\n",
      "Ground truth scores: [0. 0. 3. 0.]\n",
      "Predicted scores: [152 142 204 202]\n",
      "nDCG for this debate: 1.0\n",
      "\n",
      "Processing Debate ID: 13\n",
      "Ground truth scores: [0. 0. 1.]\n",
      "Predicted scores: [154 187 204]\n",
      "nDCG for this debate: 1.0\n",
      "\n",
      "Processing Debate ID: 14\n",
      "Ground truth scores: [0. 3. 0. 0. 0.]\n",
      "Predicted scores: [104 182 120 150 169]\n",
      "nDCG for this debate: 1.0\n",
      "\n",
      "Processing Debate ID: 16\n",
      "Ground truth scores: [0. 1. 4. 0. 0.]\n",
      "Predicted scores: [ 99 120 166 111 118]\n",
      "nDCG for this debate: 1.0\n",
      "\n",
      "Processing Debate ID: 18\n",
      "Ground truth scores: [1. 1. 1. 0.]\n",
      "Predicted scores: [191 202 193 175]\n",
      "nDCG for this debate: 1.0\n",
      "\n",
      "Mean nDCG across all debates: 0.902768220518225\n",
      "\n",
      "Mean nDCG - Hard Voting: 0.916\n",
      "Mean nDCG - Soft Voting: 0.903\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_ndcg_from_dataframes(ground_truth_df, predictions_df, k=None, exclude_debates=None):\n",
    "    \"\"\"\n",
    "    Calculates the mean nDCG for all debates using scikit-learn's ndcg_score, excluding specified debates.\n",
    "\n",
    "    Parameters:\n",
    "    - ground_truth_df: DataFrame containing the ground truth data.\n",
    "      Must have columns: 'debate_id', 'debater_name', 'debater_score'.\n",
    "    - predictions_df: DataFrame containing the predicted data.\n",
    "      Must have the same columns as ground_truth_df.\n",
    "    - k: Integer, optional. Defines the top-k for nDCG calculation. If None, considers all.\n",
    "    - exclude_debates: List of debate IDs to exclude from the calculation.\n",
    "\n",
    "    Returns:\n",
    "    - mean_ndcg: Mean nDCG across all debates (float).\n",
    "    \"\"\"\n",
    "    # Ensure the required columns are present\n",
    "    required_columns = ['debate_id', 'debater_name', 'debater_score']\n",
    "    for df in [ground_truth_df, predictions_df]:\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            raise ValueError(f\"Both DataFrames must contain the columns: {required_columns}\")\n",
    "\n",
    "    # Filter out excluded debates if provided\n",
    "    if exclude_debates:\n",
    "        ground_truth_df = ground_truth_df[~ground_truth_df['debate_id'].isin(exclude_debates)]\n",
    "        predictions_df = predictions_df[~predictions_df['debate_id'].isin(exclude_debates)]\n",
    "\n",
    "    # Get unique debates\n",
    "    debate_ids = ground_truth_df['debate_id'].unique()\n",
    "    ndcg_scores = []\n",
    "\n",
    "    for debate_id in debate_ids:\n",
    "        print(f\"\\nProcessing Debate ID: {debate_id}\")\n",
    "\n",
    "        # Filter the scores for the current debate\n",
    "        ground_truth_scores = (\n",
    "            ground_truth_df[ground_truth_df['debate_id'] == debate_id]\n",
    "            .sort_values(by='debater_name')['debater_score']\n",
    "            .values\n",
    "        )\n",
    "        predicted_scores = (\n",
    "            predictions_df[predictions_df['debate_id'] == debate_id]\n",
    "            .sort_values(by='debater_name')['debater_score']\n",
    "            .values\n",
    "        )\n",
    "\n",
    "        # Ensure the ground truth and predictions have the same length\n",
    "        if len(ground_truth_scores) != len(predicted_scores):\n",
    "            raise ValueError(f\"Mismatch in number of scores for debate {debate_id}.\")\n",
    "        \n",
    "        print(f\"Ground truth scores: {ground_truth_scores}\")\n",
    "        print(f\"Predicted scores: {predicted_scores}\")\n",
    "\n",
    "        # Reshape scores for scikit-learn (expects 2D arrays)\n",
    "        ground_truth_scores = ground_truth_scores.reshape(1, -1)\n",
    "        predicted_scores = predicted_scores.reshape(1, -1)\n",
    "\n",
    "        # Compute nDCG using scikit-learn\n",
    "        ndcg = ndcg_score(ground_truth_scores, predicted_scores, k=k)\n",
    "        print(f\"nDCG for this debate: {ndcg}\")\n",
    "\n",
    "        ndcg_scores.append(ndcg)\n",
    "\n",
    "    # Compute mean nDCG\n",
    "    mean_ndcg = np.mean(ndcg_scores)\n",
    "    print(f\"\\nMean nDCG across all debates: {mean_ndcg}\")\n",
    "    return mean_ndcg\n",
    "\n",
    "\n",
    "# Carregar os arquivos CSV\n",
    "hard_voting_df = pd.read_csv('hard_voting.csv')\n",
    "soft_voting_df = pd.read_csv('soft_voting.csv')\n",
    "self_assessment_df = pd.read_csv('rankings_self_assessment_.csv')\n",
    "\n",
    "# Renomear as colunas para uniformizar\n",
    "hard_voting_df.rename(columns={'position': 'debater_position', 'name': 'debater_name', 'score': 'debater_score'}, inplace=True)\n",
    "soft_voting_df.rename(columns={'position': 'debater_position', 'name': 'debater_name', 'score': 'debater_score'}, inplace=True)\n",
    "\n",
    "# Excluir debates 10 e 17\n",
    "exclude_debates = [10, 17]\n",
    "\n",
    "# Calcular o nDCG para Hard Voting\n",
    "print(\"\\nCalculating nDCG for Hard Voting...\")\n",
    "mean_ndcg_hard = calculate_ndcg_from_dataframes(self_assessment_df, hard_voting_df, exclude_debates=exclude_debates)\n",
    "\n",
    "# Calcular o nDCG para Soft Voting\n",
    "print(\"\\nCalculating nDCG for Soft Voting...\")\n",
    "mean_ndcg_soft = calculate_ndcg_from_dataframes(self_assessment_df, soft_voting_df, exclude_debates=exclude_debates)\n",
    "\n",
    "# Exibir os resultados\n",
    "print(f\"\\nMean nDCG - Hard Voting: {mean_ndcg_hard:.3f}\")\n",
    "print(f\"Mean nDCG - Soft Voting: {mean_ndcg_soft:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
