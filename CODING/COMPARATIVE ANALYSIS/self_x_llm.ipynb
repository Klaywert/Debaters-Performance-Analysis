{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CÓDIGO PARA COMPARAÇÃO SELF ASSESSMENT X LLMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACURACIA WINNERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Assessment Columns: Index(['debate_id', 'debater_name', 'debater_position', 'debater_score'], dtype='object')\n",
      "Gemini Predictions Columns: Index(['debate_id', 'debater_name', 'debater_position', 'debater_score'], dtype='object')\n",
      "\n",
      "Merged DataFrame Columns: Index(['debate_id', 'debater_name_ground_truth',\n",
      "       'debater_position_ground_truth', 'debater_name_pred',\n",
      "       'debater_position_pred'],\n",
      "      dtype='object')\n",
      "\n",
      "Merged DataFrame Preview:\n",
      "    debate_id debater_name_ground_truth  debater_position_ground_truth  \\\n",
      "0          1                 Debater 2                              1   \n",
      "1          1                 Debater 2                              1   \n",
      "2          1                 Debater 2                              1   \n",
      "3          1                 Debater 2                              1   \n",
      "4          1                 Debater 4                              2   \n",
      "\n",
      "  debater_name_pred  debater_position_pred  \n",
      "0         Debater 4                      1  \n",
      "1         Debater 2                      2  \n",
      "2         Debater 3                      3  \n",
      "3         Debater 1                      4  \n",
      "4         Debater 4                      1  \n",
      "Accuracy of winners: 28.57%\n",
      "Debate ID: 1\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 4\n",
      "----------------------------------------\n",
      "Debate ID: 2\n",
      "Expected winners: Debater 4, Debater 1\n",
      "Predicted winners: Debater 5\n",
      "----------------------------------------\n",
      "Debate ID: 3\n",
      "Expected winners: Debater 4\n",
      "Predicted winners: Debater 4\n",
      "----------------------------------------\n",
      "Debate ID: 5\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 5\n",
      "----------------------------------------\n",
      "Debate ID: 6\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 7\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 4\n",
      "----------------------------------------\n",
      "Debate ID: 8\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 9\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 11\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 5\n",
      "----------------------------------------\n",
      "Debate ID: 12\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 13\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 14\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 5\n",
      "----------------------------------------\n",
      "Debate ID: 16\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 18\n",
      "Expected winners: Debater 2, Debater 3, Debater 1\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "self_assessment_df = pd.read_csv('rankings_self_assessment_.csv')\n",
    "gemini_predictions_df = pd.read_csv('prompt1_gemini_.csv')\n",
    "\n",
    "# Filter out debate_id = 10 and debate_id = 17 from both datasets\n",
    "self_assessment_df = self_assessment_df[~self_assessment_df['debate_id'].isin([10, 17])]\n",
    "gemini_predictions_df = gemini_predictions_df[~gemini_predictions_df['debate_id'].isin([10, 17])]\n",
    "\n",
    "# Print columns to check for discrepancies in column names\n",
    "print(\"Self-Assessment Columns:\", self_assessment_df.columns)\n",
    "print(\"Gemini Predictions Columns:\", gemini_predictions_df.columns)\n",
    "\n",
    "# Merge the two datasets on 'debate_id' to align the ground truth with the predictions\n",
    "merged_df = pd.merge(self_assessment_df[['debate_id', 'debater_name', 'debater_position']],\n",
    "                     gemini_predictions_df[['debate_id', 'debater_name', 'debater_position']],\n",
    "                     on='debate_id', suffixes=('_ground_truth', '_pred'))\n",
    "\n",
    "# Print the merged DataFrame to check column names\n",
    "print(\"\\nMerged DataFrame Columns:\", merged_df.columns)\n",
    "print(\"\\nMerged DataFrame Preview:\\n\", merged_df.head())\n",
    "\n",
    "# Find the winners from the ground truth\n",
    "ground_truth_winners = merged_df[merged_df['debater_position_ground_truth'] == 1]\n",
    "\n",
    "# Initialize list for accuracy calculations and details\n",
    "correct_predictions = []\n",
    "debate_details = []\n",
    "\n",
    "for debate_id, group in ground_truth_winners.groupby('debate_id'):\n",
    "    # Get the names of the ground truth winners\n",
    "    true_winners = set(group['debater_name_ground_truth'])\n",
    "    \n",
    "    # Get the predicted winners for this debate_id\n",
    "    predicted_winners = set(gemini_predictions_df[gemini_predictions_df['debate_id'] == debate_id]['debater_name'][gemini_predictions_df['debater_position'] == 1])\n",
    "    \n",
    "    # Save the details for display\n",
    "    debate_details.append({\n",
    "        'debate_id': debate_id,\n",
    "        'true_winners': true_winners,\n",
    "        'predicted_winners': predicted_winners\n",
    "    })\n",
    "    \n",
    "    # If there is an intersection between true and predicted winners, it's a correct prediction\n",
    "    if true_winners.intersection(predicted_winners):\n",
    "        correct_predictions.append(1)\n",
    "    else:\n",
    "        correct_predictions.append(0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = sum(correct_predictions) / len(correct_predictions)\n",
    "print(f'Accuracy of winners: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Print the expected and predicted winners for each debate\n",
    "for detail in debate_details:\n",
    "    print(f\"Debate ID: {detail['debate_id']}\")\n",
    "    print(f\"Expected winners: {', '.join(detail['true_winners'])}\")\n",
    "    print(f\"Predicted winners: {', '.join(detail['predicted_winners'])}\")\n",
    "    print('-' * 40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Assessment Columns: Index(['debate_id', 'debater_name', 'debater_position', 'debater_score'], dtype='object')\n",
      "GPT4o Predictions Columns: Index(['debate_id', 'debater_name', 'debater_position', 'debater_score'], dtype='object')\n",
      "\n",
      "Merged DataFrame Columns: Index(['debate_id', 'debater_name_ground_truth',\n",
      "       'debater_position_ground_truth', 'debater_name_pred',\n",
      "       'debater_position_pred'],\n",
      "      dtype='object')\n",
      "\n",
      "Merged DataFrame Preview:\n",
      "    debate_id debater_name_ground_truth  debater_position_ground_truth  \\\n",
      "0          1                 Debater 2                              1   \n",
      "1          1                 Debater 2                              1   \n",
      "2          1                 Debater 2                              1   \n",
      "3          1                 Debater 2                              1   \n",
      "4          1                 Debater 4                              2   \n",
      "\n",
      "  debater_name_pred  debater_position_pred  \n",
      "0         Debater 2                      1  \n",
      "1         Debater 4                      2  \n",
      "2         Debater 1                      3  \n",
      "3         Debater 3                      4  \n",
      "4         Debater 2                      1  \n",
      "Accuracy of winners: 71.43%\n",
      "Debate ID: 1\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 2\n",
      "Expected winners: Debater 4, Debater 1\n",
      "Predicted winners: Debater 5\n",
      "----------------------------------------\n",
      "Debate ID: 3\n",
      "Expected winners: Debater 4\n",
      "Predicted winners: Debater 4\n",
      "----------------------------------------\n",
      "Debate ID: 5\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 6\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 7\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 8\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 9\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 11\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 12\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 13\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 14\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 16\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 18\n",
      "Expected winners: Debater 2, Debater 3, Debater 1\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "self_assessment_df = pd.read_csv('rankings_self_assessment_.csv')\n",
    "gpt4o_predictions_df = pd.read_csv('prompt1_gpt4o.csv')\n",
    "\n",
    "# Filter out debate_id = 10 and debate_id = 17 from both datasets\n",
    "self_assessment_df = self_assessment_df[~self_assessment_df['debate_id'].isin([10, 17])]\n",
    "gpt4o_predictions_df = gpt4o_predictions_df[~gpt4o_predictions_df['debate_id'].isin([10, 17])]\n",
    "\n",
    "# Print columns to check for discrepancies in column names\n",
    "print(\"Self-Assessment Columns:\", self_assessment_df.columns)\n",
    "print(\"GPT4o Predictions Columns:\", gpt4o_predictions_df.columns)\n",
    "\n",
    "# Merge the two datasets on 'debate_id' to align the ground truth with the predictions\n",
    "merged_df = pd.merge(self_assessment_df[['debate_id', 'debater_name', 'debater_position']],\n",
    "                     gpt4o_predictions_df[['debate_id', 'debater_name', 'debater_position']],\n",
    "                     on='debate_id', suffixes=('_ground_truth', '_pred'))\n",
    "\n",
    "# Print the merged DataFrame to check column names\n",
    "print(\"\\nMerged DataFrame Columns:\", merged_df.columns)\n",
    "print(\"\\nMerged DataFrame Preview:\\n\", merged_df.head())\n",
    "\n",
    "# Find the winners from the ground truth\n",
    "ground_truth_winners = merged_df[merged_df['debater_position_ground_truth'] == 1]\n",
    "\n",
    "# Initialize list for accuracy calculations and details\n",
    "correct_predictions = []\n",
    "debate_details = []\n",
    "\n",
    "for debate_id, group in ground_truth_winners.groupby('debate_id'):\n",
    "    # Get the names of the ground truth winners\n",
    "    true_winners = set(group['debater_name_ground_truth'])\n",
    "    \n",
    "    # Get the predicted winners for this debate_id\n",
    "    predicted_winners = set(gpt4o_predictions_df[gpt4o_predictions_df['debate_id'] == debate_id]['debater_name'][gpt4o_predictions_df['debater_position'] == 1])\n",
    "    \n",
    "    # Save the details for display\n",
    "    debate_details.append({\n",
    "        'debate_id': debate_id,\n",
    "        'true_winners': true_winners,\n",
    "        'predicted_winners': predicted_winners\n",
    "    })\n",
    "    \n",
    "    # If there is an intersection between true and predicted winners, it's a correct prediction\n",
    "    if true_winners.intersection(predicted_winners):\n",
    "        correct_predictions.append(1)\n",
    "    else:\n",
    "        correct_predictions.append(0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = sum(correct_predictions) / len(correct_predictions)\n",
    "print(f'Accuracy of winners: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Print the expected and predicted winners for each debate\n",
    "for detail in debate_details:\n",
    "    print(f\"Debate ID: {detail['debate_id']}\")\n",
    "    print(f\"Expected winners: {', '.join(detail['true_winners'])}\")\n",
    "    print(f\"Predicted winners: {', '.join(detail['predicted_winners'])}\")\n",
    "    print('-' * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACURÁCIA - PROMPT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Assessment Columns: Index(['debate_id', 'debater_name', 'debater_position', 'debater_score'], dtype='object')\n",
      "GPT4o Predictions Columns: Index(['debate_id', 'debater_name', 'debater_position', 'debater_score'], dtype='object')\n",
      "\n",
      "Merged DataFrame Columns: Index(['debate_id', 'debater_name_ground_truth',\n",
      "       'debater_position_ground_truth', 'debater_name_pred',\n",
      "       'debater_position_pred'],\n",
      "      dtype='object')\n",
      "\n",
      "Merged DataFrame Preview:\n",
      "    debate_id debater_name_ground_truth  debater_position_ground_truth  \\\n",
      "0          1                 Debater 2                              1   \n",
      "1          1                 Debater 2                              1   \n",
      "2          1                 Debater 2                              1   \n",
      "3          1                 Debater 2                              1   \n",
      "4          1                 Debater 4                              2   \n",
      "\n",
      "  debater_name_pred  debater_position_pred  \n",
      "0         Debater 3                      1  \n",
      "1         Debater 1                      2  \n",
      "2         Debater 2                      3  \n",
      "3         Debater 4                      4  \n",
      "4         Debater 3                      1  \n",
      "Accuracy of winners: 71.43%\n",
      "Debate ID: 1\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 2\n",
      "Expected winners: Debater 4, Debater 1\n",
      "Predicted winners: Debater 5\n",
      "----------------------------------------\n",
      "Debate ID: 3\n",
      "Expected winners: Debater 4\n",
      "Predicted winners: Debater 4\n",
      "----------------------------------------\n",
      "Debate ID: 5\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 6\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 7\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 8\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 9\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 11\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 12\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 13\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 14\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 16\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 18\n",
      "Expected winners: Debater 2, Debater 3, Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "self_assessment_df = pd.read_csv('rankings_self_assessment_.csv')\n",
    "gpt4o_predictions_df = pd.read_csv('prompt2_gpt4o.csv')\n",
    "\n",
    "# Filter out debate_id = 10 and debate_id = 17 from both datasets\n",
    "self_assessment_df = self_assessment_df[~self_assessment_df['debate_id'].isin([10, 17])]\n",
    "gpt4o_predictions_df = gpt4o_predictions_df[~gpt4o_predictions_df['debate_id'].isin([10, 17])]\n",
    "\n",
    "# Print columns to check for discrepancies in column names\n",
    "print(\"Self-Assessment Columns:\", self_assessment_df.columns)\n",
    "print(\"GPT4o Predictions Columns:\", gpt4o_predictions_df.columns)\n",
    "\n",
    "# Merge the two datasets on 'debate_id' to align the ground truth with the predictions\n",
    "merged_df = pd.merge(self_assessment_df[['debate_id', 'debater_name', 'debater_position']],\n",
    "                     gpt4o_predictions_df[['debate_id', 'debater_name', 'debater_position']],\n",
    "                     on='debate_id', suffixes=('_ground_truth', '_pred'))\n",
    "\n",
    "# Print the merged DataFrame to check column names\n",
    "print(\"\\nMerged DataFrame Columns:\", merged_df.columns)\n",
    "print(\"\\nMerged DataFrame Preview:\\n\", merged_df.head())\n",
    "\n",
    "# Find the winners from the ground truth\n",
    "ground_truth_winners = merged_df[merged_df['debater_position_ground_truth'] == 1]\n",
    "\n",
    "# Initialize list for accuracy calculations and details\n",
    "correct_predictions = []\n",
    "debate_details = []\n",
    "\n",
    "for debate_id, group in ground_truth_winners.groupby('debate_id'):\n",
    "    # Get the names of the ground truth winners\n",
    "    true_winners = set(group['debater_name_ground_truth'])\n",
    "    \n",
    "    # Get the predicted winners for this debate_id\n",
    "    predicted_winners = set(gpt4o_predictions_df[gpt4o_predictions_df['debate_id'] == debate_id]['debater_name'][gpt4o_predictions_df['debater_position'] == 1])\n",
    "    \n",
    "    # Save the details for display\n",
    "    debate_details.append({\n",
    "        'debate_id': debate_id,\n",
    "        'true_winners': true_winners,\n",
    "        'predicted_winners': predicted_winners\n",
    "    })\n",
    "    \n",
    "    # If there is an intersection between true and predicted winners, it's a correct prediction\n",
    "    if true_winners.intersection(predicted_winners):\n",
    "        correct_predictions.append(1)\n",
    "    else:\n",
    "        correct_predictions.append(0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = sum(correct_predictions) / len(correct_predictions)\n",
    "print(f'Accuracy of winners: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Print the expected and predicted winners for each debate\n",
    "for detail in debate_details:\n",
    "    print(f\"Debate ID: {detail['debate_id']}\")\n",
    "    print(f\"Expected winners: {', '.join(detail['true_winners'])}\")\n",
    "    print(f\"Predicted winners: {', '.join(detail['predicted_winners'])}\")\n",
    "    print('-' * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Assessment Columns: Index(['debate_id', 'debater_name', 'debater_position', 'debater_score'], dtype='object')\n",
      "GPT4o Predictions Columns: Index(['debate_id', 'debater_name', 'debater_position', 'debater_score'], dtype='object')\n",
      "\n",
      "Merged DataFrame Columns: Index(['debate_id', 'debater_name_ground_truth',\n",
      "       'debater_position_ground_truth', 'debater_name_pred',\n",
      "       'debater_position_pred'],\n",
      "      dtype='object')\n",
      "\n",
      "Merged DataFrame Preview:\n",
      "    debate_id debater_name_ground_truth  debater_position_ground_truth  \\\n",
      "0          1                 Debater 2                              1   \n",
      "1          1                 Debater 2                              1   \n",
      "2          1                 Debater 2                              1   \n",
      "3          1                 Debater 2                              1   \n",
      "4          1                 Debater 4                              2   \n",
      "\n",
      "  debater_name_pred  debater_position_pred  \n",
      "0         Debater 2                      1  \n",
      "1         Debater 1                      2  \n",
      "2         Debater 3                      3  \n",
      "3         Debater 4                      4  \n",
      "4         Debater 2                      1  \n",
      "Accuracy of winners: 78.57%\n",
      "Debate ID: 1\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 2\n",
      "Expected winners: Debater 4, Debater 1\n",
      "Predicted winners: Debater 5\n",
      "----------------------------------------\n",
      "Debate ID: 3\n",
      "Expected winners: Debater 4\n",
      "Predicted winners: Debater 4\n",
      "----------------------------------------\n",
      "Debate ID: 5\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 6\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 7\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 4\n",
      "----------------------------------------\n",
      "Debate ID: 8\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 9\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 11\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 5\n",
      "----------------------------------------\n",
      "Debate ID: 12\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 13\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 14\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 16\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 18\n",
      "Expected winners: Debater 2, Debater 3, Debater 1\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "self_assessment_df = pd.read_csv('rankings_self_assessment_.csv')\n",
    "gpt4o_predictions_df = pd.read_csv('prompt2_gemini.csv')\n",
    "\n",
    "# Filter out debate_id = 10 and debate_id = 17 from both datasets\n",
    "self_assessment_df = self_assessment_df[~self_assessment_df['debate_id'].isin([10, 17])]\n",
    "gpt4o_predictions_df = gpt4o_predictions_df[~gpt4o_predictions_df['debate_id'].isin([10, 17])]\n",
    "\n",
    "# Print columns to check for discrepancies in column names\n",
    "print(\"Self-Assessment Columns:\", self_assessment_df.columns)\n",
    "print(\"GPT4o Predictions Columns:\", gpt4o_predictions_df.columns)\n",
    "\n",
    "# Merge the two datasets on 'debate_id' to align the ground truth with the predictions\n",
    "merged_df = pd.merge(self_assessment_df[['debate_id', 'debater_name', 'debater_position']],\n",
    "                     gpt4o_predictions_df[['debate_id', 'debater_name', 'debater_position']],\n",
    "                     on='debate_id', suffixes=('_ground_truth', '_pred'))\n",
    "\n",
    "# Print the merged DataFrame to check column names\n",
    "print(\"\\nMerged DataFrame Columns:\", merged_df.columns)\n",
    "print(\"\\nMerged DataFrame Preview:\\n\", merged_df.head())\n",
    "\n",
    "# Find the winners from the ground truth\n",
    "ground_truth_winners = merged_df[merged_df['debater_position_ground_truth'] == 1]\n",
    "\n",
    "# Initialize list for accuracy calculations and details\n",
    "correct_predictions = []\n",
    "debate_details = []\n",
    "\n",
    "for debate_id, group in ground_truth_winners.groupby('debate_id'):\n",
    "    # Get the names of the ground truth winners\n",
    "    true_winners = set(group['debater_name_ground_truth'])\n",
    "    \n",
    "    # Get the predicted winners for this debate_id\n",
    "    predicted_winners = set(gpt4o_predictions_df[gpt4o_predictions_df['debate_id'] == debate_id]['debater_name'][gpt4o_predictions_df['debater_position'] == 1])\n",
    "    \n",
    "    # Save the details for display\n",
    "    debate_details.append({\n",
    "        'debate_id': debate_id,\n",
    "        'true_winners': true_winners,\n",
    "        'predicted_winners': predicted_winners\n",
    "    })\n",
    "    \n",
    "    # If there is an intersection between true and predicted winners, it's a correct prediction\n",
    "    if true_winners.intersection(predicted_winners):\n",
    "        correct_predictions.append(1)\n",
    "    else:\n",
    "        correct_predictions.append(0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = sum(correct_predictions) / len(correct_predictions)\n",
    "print(f'Accuracy of winners: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Print the expected and predicted winners for each debate\n",
    "for detail in debate_details:\n",
    "    print(f\"Debate ID: {detail['debate_id']}\")\n",
    "    print(f\"Expected winners: {', '.join(detail['true_winners'])}\")\n",
    "    print(f\"Predicted winners: {', '.join(detail['predicted_winners'])}\")\n",
    "    print('-' * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Assessment Columns: Index(['debate_id', 'debater_name', 'debater_position', 'debater_score'], dtype='object')\n",
      "GPT4o Predictions Columns: Index(['debate_id', 'debater_name', 'debater_position', 'debater_score'], dtype='object')\n",
      "\n",
      "Merged DataFrame Columns: Index(['debate_id', 'debater_name_ground_truth',\n",
      "       'debater_position_ground_truth', 'debater_name_pred',\n",
      "       'debater_position_pred'],\n",
      "      dtype='object')\n",
      "\n",
      "Merged DataFrame Preview:\n",
      "    debate_id debater_name_ground_truth  debater_position_ground_truth  \\\n",
      "0          1                 Debater 2                              1   \n",
      "1          1                 Debater 2                              1   \n",
      "2          1                 Debater 2                              1   \n",
      "3          1                 Debater 2                              1   \n",
      "4          1                 Debater 4                              2   \n",
      "\n",
      "  debater_name_pred  debater_position_pred  \n",
      "0         Debater 2                      1  \n",
      "1         Debater 4                      1  \n",
      "2         Debater 3                      3  \n",
      "3         Debater 1                      4  \n",
      "4         Debater 2                      1  \n",
      "Accuracy of winners: 78.57%\n",
      "Debate ID: 1\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 4, Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 2\n",
      "Expected winners: Debater 4, Debater 1\n",
      "Predicted winners: Debater 5\n",
      "----------------------------------------\n",
      "Debate ID: 3\n",
      "Expected winners: Debater 4\n",
      "Predicted winners: Debater 4, Debater 5, Debater 3, Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 5\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 5, Debater 2, Debater 3, Debater 4, Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 6\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 7\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 4, Debater 2, Debater 3, Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 8\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 9\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 11\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 5, Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 12\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 13\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 14\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 5\n",
      "----------------------------------------\n",
      "Debate ID: 16\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 5, Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 18\n",
      "Expected winners: Debater 2, Debater 3, Debater 1\n",
      "Predicted winners: Debater 2, Debater 3, Debater 1\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "self_assessment_df = pd.read_csv('rankings_self_assessment_.csv')\n",
    "gpt4o_predictions_df = pd.read_csv('prompt3_gemini.csv')\n",
    "\n",
    "# Filter out debate_id = 10 and debate_id = 17 from both datasets\n",
    "self_assessment_df = self_assessment_df[~self_assessment_df['debate_id'].isin([10, 17])]\n",
    "gpt4o_predictions_df = gpt4o_predictions_df[~gpt4o_predictions_df['debate_id'].isin([10, 17])]\n",
    "\n",
    "# Print columns to check for discrepancies in column names\n",
    "print(\"Self-Assessment Columns:\", self_assessment_df.columns)\n",
    "print(\"GPT4o Predictions Columns:\", gpt4o_predictions_df.columns)\n",
    "\n",
    "# Merge the two datasets on 'debate_id' to align the ground truth with the predictions\n",
    "merged_df = pd.merge(self_assessment_df[['debate_id', 'debater_name', 'debater_position']],\n",
    "                     gpt4o_predictions_df[['debate_id', 'debater_name', 'debater_position']],\n",
    "                     on='debate_id', suffixes=('_ground_truth', '_pred'))\n",
    "\n",
    "# Print the merged DataFrame to check column names\n",
    "print(\"\\nMerged DataFrame Columns:\", merged_df.columns)\n",
    "print(\"\\nMerged DataFrame Preview:\\n\", merged_df.head())\n",
    "\n",
    "# Find the winners from the ground truth\n",
    "ground_truth_winners = merged_df[merged_df['debater_position_ground_truth'] == 1]\n",
    "\n",
    "# Initialize list for accuracy calculations and details\n",
    "correct_predictions = []\n",
    "debate_details = []\n",
    "\n",
    "for debate_id, group in ground_truth_winners.groupby('debate_id'):\n",
    "    # Get the names of the ground truth winners\n",
    "    true_winners = set(group['debater_name_ground_truth'])\n",
    "    \n",
    "    # Get the predicted winners for this debate_id\n",
    "    predicted_winners = set(gpt4o_predictions_df[gpt4o_predictions_df['debate_id'] == debate_id]['debater_name'][gpt4o_predictions_df['debater_position'] == 1])\n",
    "    \n",
    "    # Save the details for display\n",
    "    debate_details.append({\n",
    "        'debate_id': debate_id,\n",
    "        'true_winners': true_winners,\n",
    "        'predicted_winners': predicted_winners\n",
    "    })\n",
    "    \n",
    "    # If there is an intersection between true and predicted winners, it's a correct prediction\n",
    "    if true_winners.intersection(predicted_winners):\n",
    "        correct_predictions.append(1)\n",
    "    else:\n",
    "        correct_predictions.append(0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = sum(correct_predictions) / len(correct_predictions)\n",
    "print(f'Accuracy of winners: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Print the expected and predicted winners for each debate\n",
    "for detail in debate_details:\n",
    "    print(f\"Debate ID: {detail['debate_id']}\")\n",
    "    print(f\"Expected winners: {', '.join(detail['true_winners'])}\")\n",
    "    print(f\"Predicted winners: {', '.join(detail['predicted_winners'])}\")\n",
    "    print('-' * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Assessment Columns: Index(['debate_id', 'debater_name', 'debater_position', 'debater_score'], dtype='object')\n",
      "GPT4o Predictions Columns: Index(['debate_id', 'debater_name', 'debater_position', 'debater_score'], dtype='object')\n",
      "\n",
      "Merged DataFrame Columns: Index(['debate_id', 'debater_name_ground_truth',\n",
      "       'debater_position_ground_truth', 'debater_name_pred',\n",
      "       'debater_position_pred'],\n",
      "      dtype='object')\n",
      "\n",
      "Merged DataFrame Preview:\n",
      "    debate_id debater_name_ground_truth  debater_position_ground_truth  \\\n",
      "0          1                 Debater 2                              1   \n",
      "1          1                 Debater 2                              1   \n",
      "2          1                 Debater 2                              1   \n",
      "3          1                 Debater 2                              1   \n",
      "4          1                 Debater 4                              2   \n",
      "\n",
      "  debater_name_pred  debater_position_pred  \n",
      "0         Debater 1                      1  \n",
      "1         Debater 2                      1  \n",
      "2         Debater 3                      1  \n",
      "3         Debater 4                      1  \n",
      "4         Debater 1                      1  \n",
      "Accuracy of winners: 85.71%\n",
      "Debate ID: 1\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 4, Debater 2, Debater 3, Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 2\n",
      "Expected winners: Debater 4, Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 3\n",
      "Expected winners: Debater 4\n",
      "Predicted winners: Debater 4\n",
      "----------------------------------------\n",
      "Debate ID: 5\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3, Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 6\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 7\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 4, Debater 2, Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 8\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 9\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 11\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 12\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3, Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 13\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 2, Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 14\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 5\n",
      "----------------------------------------\n",
      "Debate ID: 16\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 18\n",
      "Expected winners: Debater 2, Debater 3, Debater 1\n",
      "Predicted winners: Debater 4, Debater 2, Debater 3, Debater 1\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "self_assessment_df = pd.read_csv('rankings_self_assessment_.csv')\n",
    "gpt4o_predictions_df = pd.read_csv('prompt3_gpt4o.csv')\n",
    "\n",
    "# Filter out debate_id = 10 and debate_id = 17 from both datasets\n",
    "self_assessment_df = self_assessment_df[~self_assessment_df['debate_id'].isin([10, 17])]\n",
    "gpt4o_predictions_df = gpt4o_predictions_df[~gpt4o_predictions_df['debate_id'].isin([10, 17])]\n",
    "\n",
    "# Print columns to check for discrepancies in column names\n",
    "print(\"Self-Assessment Columns:\", self_assessment_df.columns)\n",
    "print(\"GPT4o Predictions Columns:\", gpt4o_predictions_df.columns)\n",
    "\n",
    "# Merge the two datasets on 'debate_id' to align the ground truth with the predictions\n",
    "merged_df = pd.merge(self_assessment_df[['debate_id', 'debater_name', 'debater_position']],\n",
    "                     gpt4o_predictions_df[['debate_id', 'debater_name', 'debater_position']],\n",
    "                     on='debate_id', suffixes=('_ground_truth', '_pred'))\n",
    "\n",
    "# Print the merged DataFrame to check column names\n",
    "print(\"\\nMerged DataFrame Columns:\", merged_df.columns)\n",
    "print(\"\\nMerged DataFrame Preview:\\n\", merged_df.head())\n",
    "\n",
    "# Find the winners from the ground truth\n",
    "ground_truth_winners = merged_df[merged_df['debater_position_ground_truth'] == 1]\n",
    "\n",
    "# Initialize list for accuracy calculations and details\n",
    "correct_predictions = []\n",
    "debate_details = []\n",
    "\n",
    "for debate_id, group in ground_truth_winners.groupby('debate_id'):\n",
    "    # Get the names of the ground truth winners\n",
    "    true_winners = set(group['debater_name_ground_truth'])\n",
    "    \n",
    "    # Get the predicted winners for this debate_id\n",
    "    predicted_winners = set(gpt4o_predictions_df[gpt4o_predictions_df['debate_id'] == debate_id]['debater_name'][gpt4o_predictions_df['debater_position'] == 1])\n",
    "    \n",
    "    # Save the details for display\n",
    "    debate_details.append({\n",
    "        'debate_id': debate_id,\n",
    "        'true_winners': true_winners,\n",
    "        'predicted_winners': predicted_winners\n",
    "    })\n",
    "    \n",
    "    # If there is an intersection between true and predicted winners, it's a correct prediction\n",
    "    if true_winners.intersection(predicted_winners):\n",
    "        correct_predictions.append(1)\n",
    "    else:\n",
    "        correct_predictions.append(0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = sum(correct_predictions) / len(correct_predictions)\n",
    "print(f'Accuracy of winners: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Print the expected and predicted winners for each debate\n",
    "for detail in debate_details:\n",
    "    print(f\"Debate ID: {detail['debate_id']}\")\n",
    "    print(f\"Expected winners: {', '.join(detail['true_winners'])}\")\n",
    "    print(f\"Predicted winners: {', '.join(detail['predicted_winners'])}\")\n",
    "    print('-' * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Assessment Columns: Index(['debate_id', 'debater_name', 'debater_position', 'debater_score'], dtype='object')\n",
      "GPT4o Predictions Columns: Index(['debate_id', 'debater_name', 'debater_position', 'debater_score'], dtype='object')\n",
      "\n",
      "Merged DataFrame Columns: Index(['debate_id', 'debater_name_ground_truth',\n",
      "       'debater_position_ground_truth', 'debater_name_pred',\n",
      "       'debater_position_pred'],\n",
      "      dtype='object')\n",
      "\n",
      "Merged DataFrame Preview:\n",
      "    debate_id debater_name_ground_truth  debater_position_ground_truth  \\\n",
      "0          1                 Debater 2                              1   \n",
      "1          1                 Debater 2                              1   \n",
      "2          1                 Debater 2                              1   \n",
      "3          1                 Debater 2                              1   \n",
      "4          1                 Debater 4                              2   \n",
      "\n",
      "  debater_name_pred  debater_position_pred  \n",
      "0         Debater 2                      1  \n",
      "1         Debater 3                      2  \n",
      "2         Debater 1                      3  \n",
      "3         Debater 4                      4  \n",
      "4         Debater 2                      1  \n",
      "Accuracy of winners: 85.71%\n",
      "Debate ID: 1\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 2\n",
      "Expected winners: Debater 4, Debater 1\n",
      "Predicted winners: Debater 5\n",
      "----------------------------------------\n",
      "Debate ID: 3\n",
      "Expected winners: Debater 4\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 5\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 5, Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 6\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 7\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 8\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 9\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 11\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 12\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 13\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 14\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 16\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 18\n",
      "Expected winners: Debater 2, Debater 3, Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "self_assessment_df = pd.read_csv('rankings_self_assessment_.csv')\n",
    "gpt4o_predictions_df = pd.read_csv('prompt4_gpt4o.csv')\n",
    "\n",
    "# Filter out debate_id = 10 and debate_id = 17 from both datasets\n",
    "self_assessment_df = self_assessment_df[~self_assessment_df['debate_id'].isin([10, 17])]\n",
    "gpt4o_predictions_df = gpt4o_predictions_df[~gpt4o_predictions_df['debate_id'].isin([10, 17])]\n",
    "\n",
    "# Print columns to check for discrepancies in column names\n",
    "print(\"Self-Assessment Columns:\", self_assessment_df.columns)\n",
    "print(\"GPT4o Predictions Columns:\", gpt4o_predictions_df.columns)\n",
    "\n",
    "# Merge the two datasets on 'debate_id' to align the ground truth with the predictions\n",
    "merged_df = pd.merge(self_assessment_df[['debate_id', 'debater_name', 'debater_position']],\n",
    "                     gpt4o_predictions_df[['debate_id', 'debater_name', 'debater_position']],\n",
    "                     on='debate_id', suffixes=('_ground_truth', '_pred'))\n",
    "\n",
    "# Print the merged DataFrame to check column names\n",
    "print(\"\\nMerged DataFrame Columns:\", merged_df.columns)\n",
    "print(\"\\nMerged DataFrame Preview:\\n\", merged_df.head())\n",
    "\n",
    "# Find the winners from the ground truth\n",
    "ground_truth_winners = merged_df[merged_df['debater_position_ground_truth'] == 1]\n",
    "\n",
    "# Initialize list for accuracy calculations and details\n",
    "correct_predictions = []\n",
    "debate_details = []\n",
    "\n",
    "for debate_id, group in ground_truth_winners.groupby('debate_id'):\n",
    "    # Get the names of the ground truth winners\n",
    "    true_winners = set(group['debater_name_ground_truth'])\n",
    "    \n",
    "    # Get the predicted winners for this debate_id\n",
    "    predicted_winners = set(gpt4o_predictions_df[gpt4o_predictions_df['debate_id'] == debate_id]['debater_name'][gpt4o_predictions_df['debater_position'] == 1])\n",
    "    \n",
    "    # Save the details for display\n",
    "    debate_details.append({\n",
    "        'debate_id': debate_id,\n",
    "        'true_winners': true_winners,\n",
    "        'predicted_winners': predicted_winners\n",
    "    })\n",
    "    \n",
    "    # If there is an intersection between true and predicted winners, it's a correct prediction\n",
    "    if true_winners.intersection(predicted_winners):\n",
    "        correct_predictions.append(1)\n",
    "    else:\n",
    "        correct_predictions.append(0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = sum(correct_predictions) / len(correct_predictions)\n",
    "print(f'Accuracy of winners: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Print the expected and predicted winners for each debate\n",
    "for detail in debate_details:\n",
    "    print(f\"Debate ID: {detail['debate_id']}\")\n",
    "    print(f\"Expected winners: {', '.join(detail['true_winners'])}\")\n",
    "    print(f\"Predicted winners: {', '.join(detail['predicted_winners'])}\")\n",
    "    print('-' * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Assessment Columns: Index(['debate_id', 'debater_name', 'debater_position', 'debater_score'], dtype='object')\n",
      "Gemini Predictions Columns: Index(['debate_id', 'debater_name', 'debater_position', 'debater_score'], dtype='object')\n",
      "\n",
      "Merged DataFrame Columns: Index(['debate_id', 'debater_name_ground_truth',\n",
      "       'debater_position_ground_truth', 'debater_name_pred',\n",
      "       'debater_position_pred'],\n",
      "      dtype='object')\n",
      "\n",
      "Merged DataFrame Preview:\n",
      "    debate_id debater_name_ground_truth  debater_position_ground_truth  \\\n",
      "0          1                 Debater 2                              1   \n",
      "1          1                 Debater 2                              1   \n",
      "2          1                 Debater 2                              1   \n",
      "3          1                 Debater 2                              1   \n",
      "4          1                 Debater 4                              2   \n",
      "\n",
      "  debater_name_pred  debater_position_pred  \n",
      "0         Debater 1                      1  \n",
      "1         Debater 2                      1  \n",
      "2         Debater 3                      1  \n",
      "3         Debater 4                      1  \n",
      "4         Debater 1                      1  \n",
      "Accuracy of winners: 62.50%\n",
      "Debate ID: 1\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 4, Debater 2, Debater 3, Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 2\n",
      "Expected winners: Debater 4, Debater 1\n",
      "Predicted winners: Debater 5\n",
      "----------------------------------------\n",
      "Debate ID: 3\n",
      "Expected winners: Debater 4\n",
      "Predicted winners: Debater 4\n",
      "----------------------------------------\n",
      "Debate ID: 5\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 6\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 7\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 4, Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 8\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 9\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 10\n",
      "Expected winners: Debater 4, Debater 1\n",
      "Predicted winners: Debater 4\n",
      "----------------------------------------\n",
      "Debate ID: 11\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 12\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 13\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 3\n",
      "----------------------------------------\n",
      "Debate ID: 14\n",
      "Expected winners: Debater 2\n",
      "Predicted winners: Debater 5\n",
      "----------------------------------------\n",
      "Debate ID: 16\n",
      "Expected winners: Debater 3\n",
      "Predicted winners: Debater 2\n",
      "----------------------------------------\n",
      "Debate ID: 17\n",
      "Expected winners: Debater 1\n",
      "Predicted winners: Debater 4, Debater 1\n",
      "----------------------------------------\n",
      "Debate ID: 18\n",
      "Expected winners: Debater 2, Debater 3, Debater 1\n",
      "Predicted winners: Debater 1\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "self_assessment_df = pd.read_csv('rankings_self_assessment_.csv')\n",
    "gpt4o_predictions_df = pd.read_csv('prompt4_gemini.csv')\n",
    "\n",
    "# Print columns to check for discrepancies in column names\n",
    "print(\"Self-Assessment Columns:\", self_assessment_df.columns)\n",
    "print(\"Gemini Predictions Columns:\", gpt4o_predictions_df.columns)\n",
    "\n",
    "# Merge the two datasets on 'debate_id' to align the ground truth with the predictions\n",
    "merged_df = pd.merge(self_assessment_df[['debate_id', 'debater_name', 'debater_position']],\n",
    "                     gpt4o_predictions_df[['debate_id', 'debater_name', 'debater_position']],\n",
    "                     on='debate_id', suffixes=('_ground_truth', '_pred'))\n",
    "\n",
    "# Print the merged DataFrame to check column names\n",
    "print(\"\\nMerged DataFrame Columns:\", merged_df.columns)\n",
    "print(\"\\nMerged DataFrame Preview:\\n\", merged_df.head())\n",
    "\n",
    "# Find the winners from the ground truth\n",
    "ground_truth_winners = merged_df[merged_df['debater_position_ground_truth'] == 1]\n",
    "\n",
    "# Initialize list for accuracy calculations and details\n",
    "correct_predictions = []\n",
    "debate_details = []\n",
    "\n",
    "for debate_id, group in ground_truth_winners.groupby('debate_id'):\n",
    "    # Get the names of the ground truth winners\n",
    "    true_winners = set(group['debater_name_ground_truth'])\n",
    "    \n",
    "    # Get the predicted winners for this debate_id\n",
    "    predicted_winners = set(gpt4o_predictions_df[gpt4o_predictions_df['debate_id'] == debate_id]['debater_name'][gpt4o_predictions_df['debater_position'] == 1])\n",
    "    \n",
    "    # Save the details for display\n",
    "    debate_details.append({\n",
    "        'debate_id': debate_id,\n",
    "        'true_winners': true_winners,\n",
    "        'predicted_winners': predicted_winners\n",
    "    })\n",
    "    \n",
    "    # If there is an intersection between true and predicted winners, it's a correct prediction\n",
    "    if true_winners.intersection(predicted_winners):\n",
    "        correct_predictions.append(1)\n",
    "    else:\n",
    "        correct_predictions.append(0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = sum(correct_predictions) / len(correct_predictions)\n",
    "print(f'Accuracy of winners: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Print the expected and predicted winners for each debate\n",
    "for detail in debate_details:\n",
    "    print(f\"Debate ID: {detail['debate_id']}\")\n",
    "    print(f\"Expected winners: {', '.join(detail['true_winners'])}\")\n",
    "    print(f\"Predicted winners: {', '.join(detail['predicted_winners'])}\")\n",
    "    print('-' * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONVERTENDO JSON PARA CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON convertido e salvo em prompt4_gpt4o.csv.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "# Nome do arquivo JSON\n",
    "json_file = 'prompt4_gpt4o.json'\n",
    "\n",
    "# Carrega o JSON do arquivo\n",
    "with open(json_file, 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "# Função para transformar o JSON\n",
    "def transform_json(json_data):\n",
    "    transformed_data = []\n",
    "\n",
    "    for entry in json_data:\n",
    "        debate_id = entry[\"debate_id\"]\n",
    "\n",
    "        # Para cada debatedor no ranking, criar uma linha separada\n",
    "        for rank in entry[\"ranking\"]:\n",
    "            row = {\n",
    "                \"debate_id\": debate_id,\n",
    "                \"debater_name\": rank[\"name\"],  # Adiciona o nome do debatedor\n",
    "                \"debater_position\": rank[\"position\"],\n",
    "                \"debater_score\": rank[\"score\"]\n",
    "            }\n",
    "            transformed_data.append(row)\n",
    "\n",
    "    return transformed_data\n",
    "\n",
    "# Transformar os dados do JSON\n",
    "transformed_data = transform_json(json_data)\n",
    "\n",
    "# Obter os cabeçalhos dinamicamente\n",
    "headers = [\"debate_id\", \"debater_name\", \"debater_position\", \"debater_score\"]\n",
    "\n",
    "# Salvar no CSV\n",
    "csv_file = 'prompt4_gpt4o.csv'\n",
    "with open(csv_file, mode='w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(transformed_data)\n",
    "\n",
    "print(f\"JSON convertido e salvo em {csv_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACURACIA RANKINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_ranking_accuracy(self_assessment_df, predictions_df, excluded_debates=[10, 17]):\n",
    "    \"\"\"\n",
    "    Calcula a acurácia do ranking comparando as previsões de rankings com os rankings reais (ground truth).\n",
    "    \n",
    "    Parameters:\n",
    "    - self_assessment_df: DataFrame com as autoavaliações (ground truth).\n",
    "    - predictions_df: DataFrame com as previsões dos modelos (ex: Gemini, GPT-4).\n",
    "    - excluded_debates: Lista de debates a serem excluídos da análise (default: [10, 17]).\n",
    "    \n",
    "    Returns:\n",
    "    - ranking_accuracy: Acurácia do ranking (em porcentagem).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filtra os debates a serem excluídos\n",
    "    self_assessment_df = self_assessment_df[~self_assessment_df['debate_id'].isin(excluded_debates)]\n",
    "    predictions_df = predictions_df[~predictions_df['debate_id'].isin(excluded_debates)]\n",
    "\n",
    "    # Lista para armazenar as acurácias de ranking de cada debate\n",
    "    correct_rankings = []\n",
    "    debate_details = []\n",
    "    \n",
    "    # Variáveis para acumular o número total de acertos e debatedores\n",
    "    total_correct = 0\n",
    "    total_debaters = 0\n",
    "\n",
    "    # Para cada debate, compara o ranking completo dos debatedores\n",
    "    for debate_id in self_assessment_df['debate_id'].unique():\n",
    "        # Filtra os debatedores do debate real e do modelo\n",
    "        true_ranking = self_assessment_df[self_assessment_df['debate_id'] == debate_id][['debater_name', 'debater_position']]\n",
    "        predicted_ranking = predictions_df[predictions_df['debate_id'] == debate_id][['debater_name', 'debater_position']]\n",
    "\n",
    "        # Ordena os debatedores conforme suas posições no ranking (ground truth e previsão)\n",
    "        true_ranking_sorted = true_ranking.sort_values(by='debater_position').reset_index(drop=True)\n",
    "        predicted_ranking_sorted = predicted_ranking.sort_values(by='debater_position').reset_index(drop=True)\n",
    "\n",
    "        # Salva os detalhes do debate para exibição\n",
    "        debate_details.append({\n",
    "            'debate_id': debate_id,\n",
    "            'true_ranking': true_ranking_sorted['debater_name'].tolist(),\n",
    "            'predicted_ranking': predicted_ranking_sorted['debater_name'].tolist()\n",
    "        })\n",
    "\n",
    "        # Inicializa o contador de acertos considerando empates\n",
    "        correct_rankings_count = 0\n",
    "\n",
    "        # Compara os rankings completos (considerando a ordem exata de posições)\n",
    "        for i in range(len(true_ranking_sorted)):\n",
    "            if true_ranking_sorted.iloc[i]['debater_name'] == predicted_ranking_sorted.iloc[i]['debater_name']:\n",
    "                correct_rankings_count += 1\n",
    "\n",
    "        # Calcula a fração de debatedores classificados corretamente neste debate\n",
    "        correct_rankings.append(correct_rankings_count / len(true_ranking_sorted))\n",
    "        \n",
    "        # Acumula os valores totais\n",
    "        total_correct += correct_rankings_count\n",
    "        total_debaters += len(true_ranking_sorted)\n",
    "\n",
    "        # Exibe informações do debate\n",
    "        print(f\"Debate ID: {debate_id}\")\n",
    "        print(f\"Ranking esperado: {', '.join(true_ranking_sorted['debater_name'])}\")\n",
    "        print(f\"Ranking previsto: {', '.join(predicted_ranking_sorted['debater_name'])}\")\n",
    "        print(f\"{correct_rankings_count} acertos de {len(true_ranking_sorted)} debatedores.\\n\")\n",
    "    \n",
    "    # Calcula a acurácia média de ranking\n",
    "    ranking_accuracy = sum(correct_rankings) / len(correct_rankings) if correct_rankings else 0\n",
    "    \n",
    "    # Exibe a acurácia do ranking\n",
    "    print(f'Acurácia de ranking: {ranking_accuracy * 100:.2f}%')\n",
    "\n",
    "    # Exibe o número total de acertos dividido pelo número total de debatedores\n",
    "    print(f\"Total de acertos: {total_correct} de {total_debaters} debatedores ({(total_correct / total_debaters) * 100:.2f}%)\")\n",
    "    \n",
    "    return ranking_accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debate ID: 1\n",
      "Ranking esperado: Debater 2, Debater 4, Debater 1, Debater 3\n",
      "Ranking previsto: Debater 4, Debater 2, Debater 3, Debater 1\n",
      "0 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 2\n",
      "Ranking esperado: Debater 1, Debater 4, Debater 5, Debater 2, Debater 3\n",
      "Ranking previsto: Debater 5, Debater 4, Debater 2, Debater 1, Debater 3\n",
      "2 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 3\n",
      "Ranking esperado: Debater 4, Debater 3, Debater 1, Debater 2, Debater 5\n",
      "Ranking previsto: Debater 4, Debater 2, Debater 5, Debater 1, Debater 3\n",
      "1 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 5\n",
      "Ranking esperado: Debater 3, Debater 5, Debater 1, Debater 2, Debater 4\n",
      "Ranking previsto: Debater 5, Debater 3, Debater 2, Debater 4, Debater 1\n",
      "0 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 6\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 2, Debater 1, Debater 4, Debater 3\n",
      "0 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 7\n",
      "Ranking esperado: Debater 2, Debater 1, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 4, Debater 3, Debater 2, Debater 1\n",
      "0 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 8\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3\n",
      "Ranking previsto: Debater 2, Debater 1, Debater 3\n",
      "1 acertos de 3 debatedores.\n",
      "\n",
      "Debate ID: 9\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 2, Debater 1, Debater 4, Debater 3\n",
      "0 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 11\n",
      "Ranking esperado: Debater 3, Debater 1, Debater 2, Debater 4, Debater 5\n",
      "Ranking previsto: Debater 5, Debater 3, Debater 2, Debater 1, Debater 4\n",
      "1 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 12\n",
      "Ranking esperado: Debater 3, Debater 1, Debater 2, Debater 4\n",
      "Ranking previsto: Debater 3, Debater 4, Debater 2, Debater 1\n",
      "2 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 13\n",
      "Ranking esperado: Debater 3, Debater 1, Debater 2\n",
      "Ranking previsto: Debater 2, Debater 3, Debater 1\n",
      "0 acertos de 3 debatedores.\n",
      "\n",
      "Debate ID: 14\n",
      "Ranking esperado: Debater 2, Debater 1, Debater 3, Debater 4, Debater 5\n",
      "Ranking previsto: Debater 5, Debater 2, Debater 4, Debater 1, Debater 3\n",
      "0 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 16\n",
      "Ranking esperado: Debater 3, Debater 2, Debater 1, Debater 4, Debater 5\n",
      "Ranking previsto: Debater 3, Debater 5, Debater 2, Debater 1, Debater 4\n",
      "1 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 18\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 2, Debater 3, Debater 1, Debater 4\n",
      "1 acertos de 4 debatedores.\n",
      "\n",
      "Acurácia de ranking: 14.88%\n",
      "Total de acertos: 9 de 60 debatedores (15.00%)\n",
      "Debate ID: 1\n",
      "Ranking esperado: Debater 2, Debater 4, Debater 1, Debater 3\n",
      "Ranking previsto: Debater 2, Debater 4, Debater 1, Debater 3\n",
      "4 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 2\n",
      "Ranking esperado: Debater 1, Debater 4, Debater 5, Debater 2, Debater 3\n",
      "Ranking previsto: Debater 5, Debater 4, Debater 2, Debater 1, Debater 3\n",
      "2 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 3\n",
      "Ranking esperado: Debater 4, Debater 3, Debater 1, Debater 2, Debater 5\n",
      "Ranking previsto: Debater 4, Debater 3, Debater 1, Debater 2, Debater 5\n",
      "5 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 5\n",
      "Ranking esperado: Debater 3, Debater 5, Debater 1, Debater 2, Debater 4\n",
      "Ranking previsto: Debater 3, Debater 4, Debater 2, Debater 5, Debater 1\n",
      "1 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 6\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 3, Debater 1, Debater 2, Debater 4\n",
      "1 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 7\n",
      "Ranking esperado: Debater 2, Debater 1, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 2, Debater 1, Debater 4, Debater 3\n",
      "2 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 8\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3\n",
      "Ranking previsto: Debater 2, Debater 1, Debater 3\n",
      "1 acertos de 3 debatedores.\n",
      "\n",
      "Debate ID: 9\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 4, Debater 3\n",
      "2 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 11\n",
      "Ranking esperado: Debater 3, Debater 1, Debater 2, Debater 4, Debater 5\n",
      "Ranking previsto: Debater 3, Debater 5, Debater 1, Debater 2, Debater 4\n",
      "1 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 12\n",
      "Ranking esperado: Debater 3, Debater 1, Debater 2, Debater 4\n",
      "Ranking previsto: Debater 3, Debater 4, Debater 2, Debater 1\n",
      "2 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 13\n",
      "Ranking esperado: Debater 3, Debater 1, Debater 2\n",
      "Ranking previsto: Debater 2, Debater 3, Debater 1\n",
      "0 acertos de 3 debatedores.\n",
      "\n",
      "Debate ID: 14\n",
      "Ranking esperado: Debater 2, Debater 1, Debater 3, Debater 4, Debater 5\n",
      "Ranking previsto: Debater 2, Debater 4, Debater 5, Debater 1, Debater 3\n",
      "1 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 16\n",
      "Ranking esperado: Debater 3, Debater 2, Debater 1, Debater 4, Debater 5\n",
      "Ranking previsto: Debater 3, Debater 5, Debater 2, Debater 1, Debater 4\n",
      "1 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 18\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 2, Debater 1, Debater 3, Debater 4\n",
      "2 acertos de 4 debatedores.\n",
      "\n",
      "Acurácia de ranking: 41.31%\n",
      "Total de acertos: 25 de 60 debatedores (41.67%)\n"
     ]
    }
   ],
   "source": [
    "# Carregar os datasets\n",
    "self_assessment_df = pd.read_csv('rankings_self_assessment_.csv')\n",
    "gemini_predictions_df = pd.read_csv('prompt1_gemini.csv')\n",
    "gpt4_predictions_df = pd.read_csv('prompt1_gpt4o.csv')\n",
    "\n",
    "# Calcular acurácia de ranking para o modelo Gemini\n",
    "ranking_accuracy_gemini = calculate_ranking_accuracy(self_assessment_df, gemini_predictions_df)\n",
    "\n",
    "# Calcular acurácia de ranking para o modelo GPT-4\n",
    "ranking_accuracy_gpt4 = calculate_ranking_accuracy(self_assessment_df, gpt4_predictions_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debate ID: 1\n",
      "Ranking esperado: Debater 2, Debater 4, Debater 1, Debater 3\n",
      "Ranking previsto: Debater 2, Debater 1, Debater 3, Debater 4\n",
      "1 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 2\n",
      "Ranking esperado: Debater 1, Debater 4, Debater 5, Debater 2, Debater 3\n",
      "Ranking previsto: Debater 5, Debater 4, Debater 1, Debater 2, Debater 3\n",
      "3 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 3\n",
      "Ranking esperado: Debater 4, Debater 3, Debater 1, Debater 2, Debater 5\n",
      "Ranking previsto: Debater 4, Debater 1, Debater 2, Debater 3, Debater 5\n",
      "2 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 5\n",
      "Ranking esperado: Debater 3, Debater 5, Debater 1, Debater 2, Debater 4\n",
      "Ranking previsto: Debater 3, Debater 2, Debater 5, Debater 1, Debater 4\n",
      "2 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 6\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 1, Debater 4, Debater 2, Debater 3\n",
      "1 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 7\n",
      "Ranking esperado: Debater 2, Debater 1, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 4, Debater 2, Debater 3, Debater 1\n",
      "1 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 8\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3\n",
      "Ranking previsto: Debater 1, Debater 3, Debater 2\n",
      "1 acertos de 3 debatedores.\n",
      "\n",
      "Debate ID: 9\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 1, Debater 4, Debater 2, Debater 3\n",
      "1 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 11\n",
      "Ranking esperado: Debater 3, Debater 1, Debater 2, Debater 4, Debater 5\n",
      "Ranking previsto: Debater 5, Debater 3, Debater 1, Debater 4, Debater 2\n",
      "1 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 12\n",
      "Ranking esperado: Debater 3, Debater 1, Debater 2, Debater 4\n",
      "Ranking previsto: Debater 3, Debater 4, Debater 2, Debater 1\n",
      "2 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 13\n",
      "Ranking esperado: Debater 3, Debater 1, Debater 2\n",
      "Ranking previsto: Debater 3, Debater 2, Debater 1\n",
      "1 acertos de 3 debatedores.\n",
      "\n",
      "Debate ID: 14\n",
      "Ranking esperado: Debater 2, Debater 1, Debater 3, Debater 4, Debater 5\n",
      "Ranking previsto: Debater 2, Debater 5, Debater 4, Debater 1, Debater 3\n",
      "1 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 16\n",
      "Ranking esperado: Debater 3, Debater 2, Debater 1, Debater 4, Debater 5\n",
      "Ranking previsto: Debater 3, Debater 1, Debater 5, Debater 2, Debater 4\n",
      "1 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 18\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 2, Debater 1, Debater 3, Debater 4\n",
      "2 acertos de 4 debatedores.\n",
      "\n",
      "Acurácia de ranking: 33.33%\n",
      "Total de acertos: 20 de 60 debatedores (33.33%)\n",
      "Debate ID: 1\n",
      "Ranking esperado: Debater 2, Debater 4, Debater 1, Debater 3\n",
      "Ranking previsto: Debater 3, Debater 1, Debater 2, Debater 4\n",
      "0 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 2\n",
      "Ranking esperado: Debater 1, Debater 4, Debater 5, Debater 2, Debater 3\n",
      "Ranking previsto: Debater 5, Debater 4, Debater 1, Debater 2, Debater 3\n",
      "3 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 3\n",
      "Ranking esperado: Debater 4, Debater 3, Debater 1, Debater 2, Debater 5\n",
      "Ranking previsto: Debater 4, Debater 1, Debater 3, Debater 5, Debater 2\n",
      "1 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 5\n",
      "Ranking esperado: Debater 3, Debater 5, Debater 1, Debater 2, Debater 4\n",
      "Ranking previsto: Debater 3, Debater 2, Debater 4, Debater 5, Debater 1\n",
      "1 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 6\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 1, Debater 3, Debater 2, Debater 4\n",
      "2 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 7\n",
      "Ranking esperado: Debater 2, Debater 1, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 2, Debater 4, Debater 3, Debater 1\n",
      "2 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 8\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3\n",
      "Ranking previsto: Debater 3, Debater 1, Debater 2\n",
      "0 acertos de 3 debatedores.\n",
      "\n",
      "Debate ID: 9\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 1, Debater 4, Debater 2, Debater 3\n",
      "1 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 11\n",
      "Ranking esperado: Debater 3, Debater 1, Debater 2, Debater 4, Debater 5\n",
      "Ranking previsto: Debater 3, Debater 5, Debater 1, Debater 4, Debater 2\n",
      "2 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 12\n",
      "Ranking esperado: Debater 3, Debater 1, Debater 2, Debater 4\n",
      "Ranking previsto: Debater 3, Debater 4, Debater 1, Debater 2\n",
      "1 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 13\n",
      "Ranking esperado: Debater 3, Debater 1, Debater 2\n",
      "Ranking previsto: Debater 2, Debater 3, Debater 1\n",
      "0 acertos de 3 debatedores.\n",
      "\n",
      "Debate ID: 14\n",
      "Ranking esperado: Debater 2, Debater 1, Debater 3, Debater 4, Debater 5\n",
      "Ranking previsto: Debater 2, Debater 5, Debater 4, Debater 3, Debater 1\n",
      "1 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 16\n",
      "Ranking esperado: Debater 3, Debater 2, Debater 1, Debater 4, Debater 5\n",
      "Ranking previsto: Debater 3, Debater 5, Debater 1, Debater 4, Debater 2\n",
      "3 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 18\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "4 acertos de 4 debatedores.\n",
      "\n",
      "Acurácia de ranking: 33.57%\n",
      "Total de acertos: 21 de 60 debatedores (35.00%)\n"
     ]
    }
   ],
   "source": [
    "# Carregar os datasets\n",
    "self_assessment_df = pd.read_csv('rankings_self_assessment_.csv')\n",
    "gemini_predictions_df = pd.read_csv('prompt2_gemini.csv')\n",
    "gpt4_predictions_df = pd.read_csv('prompt2_gpt4o.csv')\n",
    "\n",
    "# Calcular acurácia de ranking para o modelo Gemini\n",
    "ranking_accuracy_gemini = calculate_ranking_accuracy(self_assessment_df, gemini_predictions_df)\n",
    "\n",
    "# Calcular acurácia de ranking para o modelo GPT-4\n",
    "ranking_accuracy_gpt4 = calculate_ranking_accuracy(self_assessment_df, gpt4_predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debate ID: 1\n",
      "Ranking esperado: Debater 2, Debater 4, Debater 1, Debater 3\n",
      "Ranking previsto: Debater 2, Debater 4, Debater 3, Debater 1\n",
      "2 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 2\n",
      "Ranking esperado: Debater 1, Debater 4, Debater 5, Debater 2, Debater 3\n",
      "Ranking previsto: Debater 5, Debater 1, Debater 3, Debater 2, Debater 4\n",
      "1 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 3\n",
      "Ranking esperado: Debater 4, Debater 3, Debater 1, Debater 2, Debater 5\n",
      "Ranking previsto: Debater 1, Debater 3, Debater 4, Debater 5, Debater 2\n",
      "1 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 5\n",
      "Ranking esperado: Debater 3, Debater 5, Debater 1, Debater 2, Debater 4\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3, Debater 4, Debater 5\n",
      "0 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 6\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "4 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 7\n",
      "Ranking esperado: Debater 2, Debater 1, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "2 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 8\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3\n",
      "Ranking previsto: Debater 1, Debater 3, Debater 2\n",
      "1 acertos de 3 debatedores.\n",
      "\n",
      "Debate ID: 9\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 1, Debater 4, Debater 2, Debater 3\n",
      "1 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 11\n",
      "Ranking esperado: Debater 3, Debater 1, Debater 2, Debater 4, Debater 5\n",
      "Ranking previsto: Debater 3, Debater 5, Debater 1, Debater 4, Debater 2\n",
      "2 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 12\n",
      "Ranking esperado: Debater 3, Debater 1, Debater 2, Debater 4\n",
      "Ranking previsto: Debater 3, Debater 4, Debater 2, Debater 1\n",
      "2 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 13\n",
      "Ranking esperado: Debater 3, Debater 1, Debater 2\n",
      "Ranking previsto: Debater 3, Debater 1, Debater 2\n",
      "3 acertos de 3 debatedores.\n",
      "\n",
      "Debate ID: 14\n",
      "Ranking esperado: Debater 2, Debater 1, Debater 3, Debater 4, Debater 5\n",
      "Ranking previsto: Debater 5, Debater 2, Debater 4, Debater 1, Debater 3\n",
      "0 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 16\n",
      "Ranking esperado: Debater 3, Debater 2, Debater 1, Debater 4, Debater 5\n",
      "Ranking previsto: Debater 1, Debater 5, Debater 4, Debater 2, Debater 3\n",
      "0 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 18\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "4 acertos de 4 debatedores.\n",
      "\n",
      "Acurácia de ranking: 42.02%\n",
      "Total de acertos: 23 de 60 debatedores (38.33%)\n",
      "Debate ID: 1\n",
      "Ranking esperado: Debater 2, Debater 4, Debater 1, Debater 3\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "0 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 2\n",
      "Ranking esperado: Debater 1, Debater 4, Debater 5, Debater 2, Debater 3\n",
      "Ranking previsto: Debater 1, Debater 4, Debater 5, Debater 2, Debater 3\n",
      "5 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 3\n",
      "Ranking esperado: Debater 4, Debater 3, Debater 1, Debater 2, Debater 5\n",
      "Ranking previsto: Debater 4, Debater 1, Debater 2, Debater 3, Debater 5\n",
      "2 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 5\n",
      "Ranking esperado: Debater 3, Debater 5, Debater 1, Debater 2, Debater 4\n",
      "Ranking previsto: Debater 1, Debater 3, Debater 2, Debater 4, Debater 5\n",
      "0 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 6\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 1, Debater 3, Debater 4, Debater 2\n",
      "1 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 7\n",
      "Ranking esperado: Debater 2, Debater 1, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 2, Debater 3, Debater 4, Debater 1\n",
      "1 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 8\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3\n",
      "3 acertos de 3 debatedores.\n",
      "\n",
      "Debate ID: 9\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 4, Debater 3\n",
      "2 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 11\n",
      "Ranking esperado: Debater 3, Debater 1, Debater 2, Debater 4, Debater 5\n",
      "Ranking previsto: Debater 1, Debater 3, Debater 5, Debater 2, Debater 4\n",
      "0 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 12\n",
      "Ranking esperado: Debater 3, Debater 1, Debater 2, Debater 4\n",
      "Ranking previsto: Debater 1, Debater 3, Debater 2, Debater 4\n",
      "2 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 13\n",
      "Ranking esperado: Debater 3, Debater 1, Debater 2\n",
      "Ranking previsto: Debater 2, Debater 3, Debater 1\n",
      "0 acertos de 3 debatedores.\n",
      "\n",
      "Debate ID: 14\n",
      "Ranking esperado: Debater 2, Debater 1, Debater 3, Debater 4, Debater 5\n",
      "Ranking previsto: Debater 5, Debater 2, Debater 4, Debater 1, Debater 3\n",
      "0 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 16\n",
      "Ranking esperado: Debater 3, Debater 2, Debater 1, Debater 4, Debater 5\n",
      "Ranking previsto: Debater 3, Debater 2, Debater 1, Debater 4, Debater 5\n",
      "5 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 18\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "4 acertos de 4 debatedores.\n",
      "\n",
      "Acurácia de ranking: 42.14%\n",
      "Total de acertos: 25 de 60 debatedores (41.67%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carregar os datasets\n",
    "self_assessment_df = pd.read_csv('rankings_self_assessment_.csv')\n",
    "gemini_predictions_df = pd.read_csv('prompt3_gemini.csv')\n",
    "gpt4_predictions_df = pd.read_csv('prompt3_gpt4o.csv')\n",
    "\n",
    "# Calcular acurácia de ranking para o modelo Gemini\n",
    "ranking_accuracy_gemini = calculate_ranking_accuracy(self_assessment_df, gemini_predictions_df)\n",
    "\n",
    "# Calcular acurácia de ranking para o modelo GPT-4\n",
    "ranking_accuracy_gpt4 = calculate_ranking_accuracy(self_assessment_df, gpt4_predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debate ID: 1\n",
      "Ranking esperado: Debater 2, Debater 4, Debater 1, Debater 3\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "0 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 2\n",
      "Ranking esperado: Debater 1, Debater 4, Debater 5, Debater 2, Debater 3\n",
      "Ranking previsto: Debater 5, Debater 4, Debater 1, Debater 2, Debater 3\n",
      "3 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 3\n",
      "Ranking esperado: Debater 4, Debater 3, Debater 1, Debater 2, Debater 5\n",
      "Ranking previsto: Debater 4, Debater 1, Debater 3, Debater 5, Debater 2\n",
      "1 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 5\n",
      "Ranking esperado: Debater 3, Debater 5, Debater 1, Debater 2, Debater 4\n",
      "Ranking previsto: Debater 2, Debater 3, Debater 1, Debater 4, Debater 5\n",
      "1 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 6\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 2, Debater 1, Debater 4, Debater 3\n",
      "0 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 7\n",
      "Ranking esperado: Debater 2, Debater 1, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 1, Debater 4, Debater 2, Debater 3\n",
      "0 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 8\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3\n",
      "Ranking previsto: Debater 1, Debater 3, Debater 2\n",
      "1 acertos de 3 debatedores.\n",
      "\n",
      "Debate ID: 9\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 1, Debater 4, Debater 2, Debater 5, Debater 6, Debater 3\n",
      "1 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 11\n",
      "Ranking esperado: Debater 3, Debater 1, Debater 2, Debater 4, Debater 5\n",
      "Ranking previsto: Debater 3, Debater 5, Debater 1, Debater 2, Debater 4\n",
      "1 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 12\n",
      "Ranking esperado: Debater 3, Debater 1, Debater 2, Debater 4\n",
      "Ranking previsto: Debater 3, Debater 4, Debater 2, Debater 1\n",
      "2 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 13\n",
      "Ranking esperado: Debater 3, Debater 1, Debater 2\n",
      "Ranking previsto: Debater 3, Debater 2, Debater 1\n",
      "1 acertos de 3 debatedores.\n",
      "\n",
      "Debate ID: 14\n",
      "Ranking esperado: Debater 2, Debater 1, Debater 3, Debater 4, Debater 5\n",
      "Ranking previsto: Debater 5, Debater 2, Debater 4, Debater 1, Debater 3\n",
      "0 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 16\n",
      "Ranking esperado: Debater 3, Debater 2, Debater 1, Debater 4, Debater 5\n",
      "Ranking previsto: Debater 2, Debater 1, Debater 3, Debater 5, Debater 4\n",
      "0 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 18\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 1, Debater 3, Debater 2, Debater 4\n",
      "2 acertos de 4 debatedores.\n",
      "\n",
      "Acurácia de ranking: 22.26%\n",
      "Total de acertos: 13 de 60 debatedores (21.67%)\n",
      "Debate ID: 1\n",
      "Ranking esperado: Debater 2, Debater 4, Debater 1, Debater 3\n",
      "Ranking previsto: Debater 2, Debater 3, Debater 1, Debater 4\n",
      "2 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 2\n",
      "Ranking esperado: Debater 1, Debater 4, Debater 5, Debater 2, Debater 3\n",
      "Ranking previsto: Debater 5, Debater 1, Debater 4, Debater 2, Debater 3\n",
      "2 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 3\n",
      "Ranking esperado: Debater 4, Debater 3, Debater 1, Debater 2, Debater 5\n",
      "Ranking previsto: Debater 1, Debater 4, Debater 3, Debater 2, Debater 5\n",
      "2 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 5\n",
      "Ranking esperado: Debater 3, Debater 5, Debater 1, Debater 2, Debater 4\n",
      "Ranking previsto: Debater 3, Debater 5, Debater 1, Debater 2, Debater 4\n",
      "5 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 6\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 1, Debater 4, Debater 3, Debater 2\n",
      "2 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 7\n",
      "Ranking esperado: Debater 2, Debater 1, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 2, Debater 3, Debater 4, Debater 1\n",
      "1 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 8\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3\n",
      "Ranking previsto: Debater 1, Debater 3, Debater 2\n",
      "1 acertos de 3 debatedores.\n",
      "\n",
      "Debate ID: 9\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 1, Debater 4, Debater 2, Debater 3\n",
      "1 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 11\n",
      "Ranking esperado: Debater 3, Debater 1, Debater 2, Debater 4, Debater 5\n",
      "Ranking previsto: Debater 3, Debater 1, Debater 5, Debater 2, Debater 4\n",
      "2 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 12\n",
      "Ranking esperado: Debater 3, Debater 1, Debater 2, Debater 4\n",
      "Ranking previsto: Debater 3, Debater 4, Debater 1, Debater 2\n",
      "1 acertos de 4 debatedores.\n",
      "\n",
      "Debate ID: 13\n",
      "Ranking esperado: Debater 3, Debater 1, Debater 2\n",
      "Ranking previsto: Debater 3, Debater 2, Debater 1\n",
      "1 acertos de 3 debatedores.\n",
      "\n",
      "Debate ID: 14\n",
      "Ranking esperado: Debater 2, Debater 1, Debater 3, Debater 4, Debater 5\n",
      "Ranking previsto: Debater 2, Debater 5, Debater 4, Debater 1, Debater 3\n",
      "1 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 16\n",
      "Ranking esperado: Debater 3, Debater 2, Debater 1, Debater 4, Debater 5\n",
      "Ranking previsto: Debater 3, Debater 1, Debater 2, Debater 5, Debater 4\n",
      "1 acertos de 5 debatedores.\n",
      "\n",
      "Debate ID: 18\n",
      "Ranking esperado: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "4 acertos de 4 debatedores.\n",
      "\n",
      "Acurácia de ranking: 42.98%\n",
      "Total de acertos: 26 de 60 debatedores (43.33%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carregar os datasets\n",
    "self_assessment_df = pd.read_csv('rankings_self_assessment_.csv')\n",
    "gemini_predictions_df = pd.read_csv('prompt4_gemini.csv')\n",
    "gpt4_predictions_df = pd.read_csv('prompt4_gpt4o.csv')\n",
    "\n",
    "# Calcular acurácia de ranking para o modelo Gemini\n",
    "ranking_accuracy_gemini = calculate_ranking_accuracy(self_assessment_df, gemini_predictions_df)\n",
    "\n",
    "# Calcular acurácia de ranking para o modelo GPT-4\n",
    "ranking_accuracy_gpt4 = calculate_ranking_accuracy(self_assessment_df, gpt4_predictions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_mrr(self_assessment_df, predictions_df, excluded_debates=[10, 17]):\n",
    "    \"\"\"\n",
    "    Calcula o Mean Reciprocal Rank (MRR) comparando as previsões de rankings com os rankings reais (ground truth).\n",
    "    \n",
    "    Parameters:\n",
    "    - self_assessment_df: DataFrame com as autoavaliações (ground truth).\n",
    "    - predictions_df: DataFrame com as previsões dos modelos (ex: Gemini, GPT-4).\n",
    "    - excluded_debates: Lista de debates a serem excluídos da análise (default: [10, 17]).\n",
    "    \n",
    "    Returns:\n",
    "    - mrr: Mean Reciprocal Rank (em decimal).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filtra os debates a serem excluídos\n",
    "    self_assessment_df = self_assessment_df[~self_assessment_df['debate_id'].isin(excluded_debates)]\n",
    "    predictions_df = predictions_df[~predictions_df['debate_id'].isin(excluded_debates)]\n",
    "\n",
    "    # Lista para armazenar os Reciprocal Ranks de cada debate\n",
    "    reciprocal_ranks = []\n",
    "\n",
    "    # Para cada debate, calcula o Reciprocal Rank\n",
    "    for debate_id in self_assessment_df['debate_id'].unique():\n",
    "        # Filtra os debatedores do debate real e do modelo\n",
    "        true_ranking = self_assessment_df[self_assessment_df['debate_id'] == debate_id][['debater_name', 'debater_position']]\n",
    "        predicted_ranking = predictions_df[predictions_df['debate_id'] == debate_id][['debater_name', 'debater_position']]\n",
    "\n",
    "        # Ordena os debatedores conforme suas posições no ranking (ground truth e previsão)\n",
    "        true_ranking_sorted = true_ranking.sort_values(by='debater_position').reset_index(drop=True)\n",
    "        predicted_ranking_sorted = predicted_ranking.sort_values(by='debater_position').reset_index(drop=True)\n",
    "\n",
    "        # Lista de vencedores no ground truth (pode haver empate)\n",
    "        true_winners = set(true_ranking_sorted[true_ranking_sorted['debater_position'] == 1]['debater_name'])\n",
    "\n",
    "        # Calcula o Reciprocal Rank\n",
    "        rr = 0\n",
    "        for rank, debater in enumerate(predicted_ranking_sorted['debater_name'], start=1):\n",
    "            if debater in true_winners:\n",
    "                rr = 1 / rank\n",
    "                break\n",
    "        \n",
    "        # Armazena o RR do debate\n",
    "        reciprocal_ranks.append(rr)\n",
    "\n",
    "        # Exibe informações do debate\n",
    "        print(f\"Debate ID: {debate_id}\")\n",
    "        print(f\"Vencedores esperados: {', '.join(true_winners)}\")\n",
    "        print(f\"Ranking previsto: {', '.join(predicted_ranking_sorted['debater_name'])}\")\n",
    "        print(f\"Reciprocal Rank: {rr:.3f}\\n\")\n",
    "\n",
    "    # Calcula o MRR\n",
    "    mrr = sum(reciprocal_ranks) / len(reciprocal_ranks) if reciprocal_ranks else 0\n",
    "    \n",
    "    # Exibe o MRR\n",
    "    print(f\"Mean Reciprocal Rank (MRR): {mrr:.3f}\")\n",
    "\n",
    "    return mrr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debate ID: 1\n",
      "Vencedores esperados: Debater 2\n",
      "Ranking previsto: Debater 4, Debater 2, Debater 3, Debater 1\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 2\n",
      "Vencedores esperados: Debater 4, Debater 1\n",
      "Ranking previsto: Debater 5, Debater 4, Debater 2, Debater 1, Debater 3\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 3\n",
      "Vencedores esperados: Debater 4\n",
      "Ranking previsto: Debater 4, Debater 2, Debater 5, Debater 1, Debater 3\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 5\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 5, Debater 3, Debater 2, Debater 4, Debater 1\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 6\n",
      "Vencedores esperados: Debater 1\n",
      "Ranking previsto: Debater 2, Debater 1, Debater 4, Debater 3\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 7\n",
      "Vencedores esperados: Debater 2\n",
      "Ranking previsto: Debater 4, Debater 3, Debater 2, Debater 1\n",
      "Reciprocal Rank: 0.333\n",
      "\n",
      "Debate ID: 8\n",
      "Vencedores esperados: Debater 1\n",
      "Ranking previsto: Debater 2, Debater 1, Debater 3\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 9\n",
      "Vencedores esperados: Debater 1\n",
      "Ranking previsto: Debater 2, Debater 1, Debater 4, Debater 3\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 11\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 5, Debater 3, Debater 2, Debater 1, Debater 4\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 12\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 3, Debater 4, Debater 2, Debater 1\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 13\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 2, Debater 3, Debater 1\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 14\n",
      "Vencedores esperados: Debater 2\n",
      "Ranking previsto: Debater 5, Debater 2, Debater 4, Debater 1, Debater 3\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 16\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 3, Debater 5, Debater 2, Debater 1, Debater 4\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 18\n",
      "Vencedores esperados: Debater 3, Debater 1, Debater 2\n",
      "Ranking previsto: Debater 2, Debater 3, Debater 1, Debater 4\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Mean Reciprocal Rank (MRR): 0.631\n"
     ]
    }
   ],
   "source": [
    "self_assessment_df = pd.read_csv(\"rankings_self_assessment_.csv\")\n",
    "predictions_df = pd.read_csv(\"prompt1_gemini.csv\")\n",
    "\n",
    "mrr = calculate_mrr(self_assessment_df, predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debate ID: 1\n",
      "Vencedores esperados: Debater 2\n",
      "Ranking previsto: Debater 2, Debater 4, Debater 1, Debater 3\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 2\n",
      "Vencedores esperados: Debater 4, Debater 1\n",
      "Ranking previsto: Debater 5, Debater 4, Debater 2, Debater 1, Debater 3\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 3\n",
      "Vencedores esperados: Debater 4\n",
      "Ranking previsto: Debater 4, Debater 3, Debater 1, Debater 2, Debater 5\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 5\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 3, Debater 4, Debater 2, Debater 5, Debater 1\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 6\n",
      "Vencedores esperados: Debater 1\n",
      "Ranking previsto: Debater 3, Debater 1, Debater 2, Debater 4\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 7\n",
      "Vencedores esperados: Debater 2\n",
      "Ranking previsto: Debater 2, Debater 1, Debater 4, Debater 3\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 8\n",
      "Vencedores esperados: Debater 1\n",
      "Ranking previsto: Debater 2, Debater 1, Debater 3\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 9\n",
      "Vencedores esperados: Debater 1\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 4, Debater 3\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 11\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 3, Debater 5, Debater 1, Debater 2, Debater 4\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 12\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 3, Debater 4, Debater 2, Debater 1\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 13\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 2, Debater 3, Debater 1\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 14\n",
      "Vencedores esperados: Debater 2\n",
      "Ranking previsto: Debater 2, Debater 4, Debater 5, Debater 1, Debater 3\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 16\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 3, Debater 5, Debater 2, Debater 1, Debater 4\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 18\n",
      "Vencedores esperados: Debater 3, Debater 1, Debater 2\n",
      "Ranking previsto: Debater 2, Debater 1, Debater 3, Debater 4\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Mean Reciprocal Rank (MRR): 0.857\n"
     ]
    }
   ],
   "source": [
    "self_assessment_df = pd.read_csv(\"rankings_self_assessment_.csv\")\n",
    "predictions_df = pd.read_csv(\"prompt1_gpt4o.csv\")\n",
    "\n",
    "mrr = calculate_mrr(self_assessment_df, predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debate ID: 1\n",
      "Vencedores esperados: Debater 2\n",
      "Ranking previsto: Debater 2, Debater 1, Debater 3, Debater 4\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 2\n",
      "Vencedores esperados: Debater 4, Debater 1\n",
      "Ranking previsto: Debater 5, Debater 4, Debater 1, Debater 2, Debater 3\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 3\n",
      "Vencedores esperados: Debater 4\n",
      "Ranking previsto: Debater 4, Debater 1, Debater 2, Debater 3, Debater 5\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 5\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 3, Debater 2, Debater 5, Debater 1, Debater 4\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 6\n",
      "Vencedores esperados: Debater 1\n",
      "Ranking previsto: Debater 1, Debater 4, Debater 2, Debater 3\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 7\n",
      "Vencedores esperados: Debater 2\n",
      "Ranking previsto: Debater 4, Debater 2, Debater 3, Debater 1\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 8\n",
      "Vencedores esperados: Debater 1\n",
      "Ranking previsto: Debater 1, Debater 3, Debater 2\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 9\n",
      "Vencedores esperados: Debater 1\n",
      "Ranking previsto: Debater 1, Debater 4, Debater 2, Debater 3\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 11\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 5, Debater 3, Debater 1, Debater 4, Debater 2\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 12\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 3, Debater 4, Debater 2, Debater 1\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 13\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 3, Debater 2, Debater 1\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 14\n",
      "Vencedores esperados: Debater 2\n",
      "Ranking previsto: Debater 2, Debater 5, Debater 4, Debater 1, Debater 3\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 16\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 3, Debater 1, Debater 5, Debater 2, Debater 4\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 18\n",
      "Vencedores esperados: Debater 3, Debater 1, Debater 2\n",
      "Ranking previsto: Debater 2, Debater 1, Debater 3, Debater 4\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Mean Reciprocal Rank (MRR): 0.893\n"
     ]
    }
   ],
   "source": [
    "self_assessment_df = pd.read_csv(\"rankings_self_assessment_.csv\")\n",
    "predictions_df = pd.read_csv(\"prompt2_gemini.csv\")\n",
    "\n",
    "mrr = calculate_mrr(self_assessment_df, predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debate ID: 1\n",
      "Vencedores esperados: Debater 2\n",
      "Ranking previsto: Debater 3, Debater 1, Debater 2, Debater 4\n",
      "Reciprocal Rank: 0.333\n",
      "\n",
      "Debate ID: 2\n",
      "Vencedores esperados: Debater 4, Debater 1\n",
      "Ranking previsto: Debater 5, Debater 4, Debater 1, Debater 2, Debater 3\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 3\n",
      "Vencedores esperados: Debater 4\n",
      "Ranking previsto: Debater 4, Debater 1, Debater 3, Debater 5, Debater 2\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 5\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 3, Debater 2, Debater 4, Debater 5, Debater 1\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 6\n",
      "Vencedores esperados: Debater 1\n",
      "Ranking previsto: Debater 1, Debater 3, Debater 2, Debater 4\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 7\n",
      "Vencedores esperados: Debater 2\n",
      "Ranking previsto: Debater 2, Debater 4, Debater 3, Debater 1\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 8\n",
      "Vencedores esperados: Debater 1\n",
      "Ranking previsto: Debater 3, Debater 1, Debater 2\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 9\n",
      "Vencedores esperados: Debater 1\n",
      "Ranking previsto: Debater 1, Debater 4, Debater 2, Debater 3\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 11\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 3, Debater 5, Debater 1, Debater 4, Debater 2\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 12\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 3, Debater 4, Debater 1, Debater 2\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 13\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 2, Debater 3, Debater 1\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 14\n",
      "Vencedores esperados: Debater 2\n",
      "Ranking previsto: Debater 2, Debater 5, Debater 4, Debater 3, Debater 1\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 16\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 3, Debater 5, Debater 1, Debater 4, Debater 2\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 18\n",
      "Vencedores esperados: Debater 3, Debater 1, Debater 2\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Mean Reciprocal Rank (MRR): 0.845\n"
     ]
    }
   ],
   "source": [
    "self_assessment_df = pd.read_csv(\"rankings_self_assessment_.csv\")\n",
    "predictions_df = pd.read_csv(\"prompt2_gpt4o.csv\")\n",
    "\n",
    "mrr = calculate_mrr(self_assessment_df, predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debate ID: 1\n",
      "Vencedores esperados: Debater 2\n",
      "Ranking previsto: Debater 2, Debater 4, Debater 3, Debater 1\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 2\n",
      "Vencedores esperados: Debater 4, Debater 1\n",
      "Ranking previsto: Debater 5, Debater 1, Debater 3, Debater 2, Debater 4\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 3\n",
      "Vencedores esperados: Debater 4\n",
      "Ranking previsto: Debater 1, Debater 3, Debater 4, Debater 5, Debater 2\n",
      "Reciprocal Rank: 0.333\n",
      "\n",
      "Debate ID: 5\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3, Debater 4, Debater 5\n",
      "Reciprocal Rank: 0.333\n",
      "\n",
      "Debate ID: 6\n",
      "Vencedores esperados: Debater 1\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 7\n",
      "Vencedores esperados: Debater 2\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 8\n",
      "Vencedores esperados: Debater 1\n",
      "Ranking previsto: Debater 1, Debater 3, Debater 2\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 9\n",
      "Vencedores esperados: Debater 1\n",
      "Ranking previsto: Debater 1, Debater 4, Debater 2, Debater 3\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 11\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 3, Debater 5, Debater 1, Debater 4, Debater 2\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 12\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 3, Debater 4, Debater 2, Debater 1\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 13\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 3, Debater 1, Debater 2\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 14\n",
      "Vencedores esperados: Debater 2\n",
      "Ranking previsto: Debater 5, Debater 2, Debater 4, Debater 1, Debater 3\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 16\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 1, Debater 5, Debater 4, Debater 2, Debater 3\n",
      "Reciprocal Rank: 0.200\n",
      "\n",
      "Debate ID: 18\n",
      "Vencedores esperados: Debater 3, Debater 1, Debater 2\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Mean Reciprocal Rank (MRR): 0.740\n"
     ]
    }
   ],
   "source": [
    "self_assessment_df = pd.read_csv(\"rankings_self_assessment_.csv\")\n",
    "predictions_df = pd.read_csv(\"prompt3_gemini.csv\")\n",
    "\n",
    "mrr = calculate_mrr(self_assessment_df, predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debate ID: 1\n",
      "Vencedores esperados: Debater 2\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 2\n",
      "Vencedores esperados: Debater 4, Debater 1\n",
      "Ranking previsto: Debater 1, Debater 4, Debater 5, Debater 2, Debater 3\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 3\n",
      "Vencedores esperados: Debater 4\n",
      "Ranking previsto: Debater 4, Debater 1, Debater 2, Debater 3, Debater 5\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 5\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 1, Debater 3, Debater 2, Debater 4, Debater 5\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 6\n",
      "Vencedores esperados: Debater 1\n",
      "Ranking previsto: Debater 1, Debater 3, Debater 4, Debater 2\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 7\n",
      "Vencedores esperados: Debater 2\n",
      "Ranking previsto: Debater 2, Debater 3, Debater 4, Debater 1\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 8\n",
      "Vencedores esperados: Debater 1\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 9\n",
      "Vencedores esperados: Debater 1\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 4, Debater 3\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 11\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 1, Debater 3, Debater 5, Debater 2, Debater 4\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 12\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 1, Debater 3, Debater 2, Debater 4\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 13\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 2, Debater 3, Debater 1\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 14\n",
      "Vencedores esperados: Debater 2\n",
      "Ranking previsto: Debater 5, Debater 2, Debater 4, Debater 1, Debater 3\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 16\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 3, Debater 2, Debater 1, Debater 4, Debater 5\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 18\n",
      "Vencedores esperados: Debater 3, Debater 1, Debater 2\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Mean Reciprocal Rank (MRR): 0.786\n"
     ]
    }
   ],
   "source": [
    "self_assessment_df = pd.read_csv(\"rankings_self_assessment_.csv\")\n",
    "predictions_df = pd.read_csv(\"prompt3_gpt4o.csv\")\n",
    "\n",
    "mrr = calculate_mrr(self_assessment_df, predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debate ID: 1\n",
      "Vencedores esperados: Debater 2\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 2\n",
      "Vencedores esperados: Debater 4, Debater 1\n",
      "Ranking previsto: Debater 5, Debater 4, Debater 1, Debater 2, Debater 3\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 3\n",
      "Vencedores esperados: Debater 4\n",
      "Ranking previsto: Debater 4, Debater 1, Debater 3, Debater 5, Debater 2\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 5\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 2, Debater 3, Debater 1, Debater 4, Debater 5\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 6\n",
      "Vencedores esperados: Debater 1\n",
      "Ranking previsto: Debater 2, Debater 1, Debater 4, Debater 3\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 7\n",
      "Vencedores esperados: Debater 2\n",
      "Ranking previsto: Debater 1, Debater 4, Debater 2, Debater 3\n",
      "Reciprocal Rank: 0.333\n",
      "\n",
      "Debate ID: 8\n",
      "Vencedores esperados: Debater 1\n",
      "Ranking previsto: Debater 1, Debater 3, Debater 2\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 9\n",
      "Vencedores esperados: Debater 1\n",
      "Ranking previsto: Debater 1, Debater 4, Debater 2, Debater 5, Debater 6, Debater 3\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 11\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 3, Debater 5, Debater 1, Debater 2, Debater 4\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 12\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 3, Debater 4, Debater 2, Debater 1\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 13\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 3, Debater 2, Debater 1\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 14\n",
      "Vencedores esperados: Debater 2\n",
      "Ranking previsto: Debater 5, Debater 2, Debater 4, Debater 1, Debater 3\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 16\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 2, Debater 1, Debater 3, Debater 5, Debater 4\n",
      "Reciprocal Rank: 0.333\n",
      "\n",
      "Debate ID: 18\n",
      "Vencedores esperados: Debater 3, Debater 1, Debater 2\n",
      "Ranking previsto: Debater 1, Debater 3, Debater 2, Debater 4\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Mean Reciprocal Rank (MRR): 0.726\n"
     ]
    }
   ],
   "source": [
    "self_assessment_df = pd.read_csv(\"rankings_self_assessment_.csv\")\n",
    "predictions_df = pd.read_csv(\"prompt4_gemini.csv\")\n",
    "\n",
    "mrr = calculate_mrr(self_assessment_df, predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debate ID: 1\n",
      "Vencedores esperados: Debater 2\n",
      "Ranking previsto: Debater 2, Debater 3, Debater 1, Debater 4\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 2\n",
      "Vencedores esperados: Debater 4, Debater 1\n",
      "Ranking previsto: Debater 5, Debater 1, Debater 4, Debater 2, Debater 3\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 3\n",
      "Vencedores esperados: Debater 4\n",
      "Ranking previsto: Debater 1, Debater 4, Debater 3, Debater 2, Debater 5\n",
      "Reciprocal Rank: 0.500\n",
      "\n",
      "Debate ID: 5\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 3, Debater 5, Debater 1, Debater 2, Debater 4\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 6\n",
      "Vencedores esperados: Debater 1\n",
      "Ranking previsto: Debater 1, Debater 4, Debater 3, Debater 2\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 7\n",
      "Vencedores esperados: Debater 2\n",
      "Ranking previsto: Debater 2, Debater 3, Debater 4, Debater 1\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 8\n",
      "Vencedores esperados: Debater 1\n",
      "Ranking previsto: Debater 1, Debater 3, Debater 2\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 9\n",
      "Vencedores esperados: Debater 1\n",
      "Ranking previsto: Debater 1, Debater 4, Debater 2, Debater 3\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 11\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 3, Debater 1, Debater 5, Debater 2, Debater 4\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 12\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 3, Debater 4, Debater 1, Debater 2\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 13\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 3, Debater 2, Debater 1\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 14\n",
      "Vencedores esperados: Debater 2\n",
      "Ranking previsto: Debater 2, Debater 5, Debater 4, Debater 1, Debater 3\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 16\n",
      "Vencedores esperados: Debater 3\n",
      "Ranking previsto: Debater 3, Debater 1, Debater 2, Debater 5, Debater 4\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Debate ID: 18\n",
      "Vencedores esperados: Debater 3, Debater 1, Debater 2\n",
      "Ranking previsto: Debater 1, Debater 2, Debater 3, Debater 4\n",
      "Reciprocal Rank: 1.000\n",
      "\n",
      "Mean Reciprocal Rank (MRR): 0.929\n"
     ]
    }
   ],
   "source": [
    "self_assessment_df = pd.read_csv(\"rankings_self_assessment_.csv\")\n",
    "predictions_df = pd.read_csv(\"prompt4_gpt4o.csv\")\n",
    "\n",
    "mrr = calculate_mrr(self_assessment_df, predictions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_ndcg_from_dataframes(ground_truth_df, predictions_df, k=None, exclude_debates=None):\n",
    "    \"\"\"\n",
    "    Calculates the mean nDCG for all debates using scikit-learn's ndcg_score, excluding specified debates.\n",
    "\n",
    "    Parameters:\n",
    "    - ground_truth_df: DataFrame containing the ground truth data.\n",
    "      Must have columns: 'debate_id', 'debater_name', 'debater_score'.\n",
    "    - predictions_df: DataFrame containing the predicted data.\n",
    "      Must have the same columns as ground_truth_df.\n",
    "    - k: Integer, optional. Defines the top-k for nDCG calculation. If None, considers all.\n",
    "    - exclude_debates: List of debate IDs to exclude from the calculation.\n",
    "\n",
    "    Returns:\n",
    "    - mean_ndcg: Mean nDCG across all debates (float).\n",
    "    \"\"\"\n",
    "    # Ensure the required columns are present\n",
    "    required_columns = ['debate_id', 'debater_name', 'debater_score']\n",
    "    for df in [ground_truth_df, predictions_df]:\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            raise ValueError(f\"Both DataFrames must contain the columns: {required_columns}\")\n",
    "\n",
    "    # Filter out excluded debates if provided\n",
    "    if exclude_debates:\n",
    "        ground_truth_df = ground_truth_df[~ground_truth_df['debate_id'].isin(exclude_debates)]\n",
    "        predictions_df = predictions_df[~predictions_df['debate_id'].isin(exclude_debates)]\n",
    "\n",
    "    # Get unique debates\n",
    "    debate_ids = ground_truth_df['debate_id'].unique()\n",
    "    ndcg_scores = []\n",
    "\n",
    "    for debate_id in debate_ids:\n",
    "        print(f\"\\nProcessing Debate ID: {debate_id}\")\n",
    "\n",
    "        # Filter the scores for the current debate\n",
    "        ground_truth_scores = (\n",
    "            ground_truth_df[ground_truth_df['debate_id'] == debate_id]\n",
    "            .sort_values(by='debater_name')['debater_score']\n",
    "            .values\n",
    "        )\n",
    "        predicted_scores = (\n",
    "            predictions_df[predictions_df['debate_id'] == debate_id]\n",
    "            .sort_values(by='debater_name')['debater_score']\n",
    "            .values\n",
    "        )\n",
    "\n",
    "        # Ensure the ground truth and predictions have the same length\n",
    "        if len(ground_truth_scores) != len(predicted_scores):\n",
    "            raise ValueError(f\"Mismatch in number of scores for debate {debate_id}.\")\n",
    "        \n",
    "        print(f\"Ground truth scores: {ground_truth_scores}\")\n",
    "        print(f\"Predicted scores: {predicted_scores}\")\n",
    "\n",
    "        # Reshape scores for scikit-learn (expects 2D arrays)\n",
    "        ground_truth_scores = ground_truth_scores.reshape(1, -1)\n",
    "        predicted_scores = predicted_scores.reshape(1, -1)\n",
    "\n",
    "        # Compute nDCG using scikit-learn\n",
    "        ndcg = ndcg_score(ground_truth_scores, predicted_scores, k=k)\n",
    "        print(f\"nDCG for this debate: {ndcg}\")\n",
    "\n",
    "        ndcg_scores.append(ndcg)\n",
    "\n",
    "    # Compute mean nDCG\n",
    "    mean_ndcg = np.mean(ndcg_scores)\n",
    "    print(f\"\\nMean nDCG across all debates: {mean_ndcg}\")\n",
    "    return mean_ndcg\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Debate ID: 1\n",
      "Ground truth scores: [0. 2. 0. 1.]\n",
      "Predicted scores: [7.5 8.8 8.2 9.1]\n",
      "nDCG for this debate: 0.8597186998521971\n",
      "\n",
      "Processing Debate ID: 2\n",
      "Ground truth scores: [2. 0. 0. 2. 1.]\n",
      "Predicted scores: [7.5 8.  6.5 8.5 9. ]\n",
      "nDCG for this debate: 0.8302310645465177\n",
      "\n",
      "Processing Debate ID: 3\n",
      "Ground truth scores: [0. 0. 1. 4. 0.]\n",
      "Predicted scores: [7.5 8.  7.  8.5 8. ]\n",
      "nDCG for this debate: 0.9472941807962689\n",
      "\n",
      "Processing Debate ID: 5\n",
      "Ground truth scores: [0. 0. 2. 0. 1.]\n",
      "Predicted scores: [6.5 7.5 8.  7.  8.5]\n",
      "nDCG for this debate: 0.8597186998521971\n",
      "\n",
      "Processing Debate ID: 6\n",
      "Ground truth scores: [3. 0. 0. 0.]\n",
      "Predicted scores: [8.5 9.  7.5 8. ]\n",
      "nDCG for this debate: 0.6309297535714573\n",
      "\n",
      "Processing Debate ID: 7\n",
      "Ground truth scores: [0. 3. 0. 0.]\n",
      "Predicted scores: [7.5 8.  8.5 9. ]\n",
      "nDCG for this debate: 0.5\n",
      "\n",
      "Processing Debate ID: 8\n",
      "Ground truth scores: [1. 0. 0.]\n",
      "Predicted scores: [8.5 9.  7.5]\n",
      "nDCG for this debate: 0.6309297535714573\n",
      "\n",
      "Processing Debate ID: 9\n",
      "Ground truth scores: [2. 1. 0. 0.]\n",
      "Predicted scores: [8.5 9.9 7.  8. ]\n",
      "nDCG for this debate: 0.8597186998521971\n",
      "\n",
      "Processing Debate ID: 11\n",
      "Ground truth scores: [1. 0. 2. 0. 0.]\n",
      "Predicted scores: [7.5 8.  8.5 7.  9. ]\n",
      "nDCG for this debate: 0.6433224083306326\n",
      "\n",
      "Processing Debate ID: 12\n",
      "Ground truth scores: [0. 0. 3. 0.]\n",
      "Predicted scores: [7.  8.5 9.2 8.8]\n",
      "nDCG for this debate: 1.0\n",
      "\n",
      "Processing Debate ID: 13\n",
      "Ground truth scores: [0. 0. 1.]\n",
      "Predicted scores: [7.5 9.  8.5]\n",
      "nDCG for this debate: 0.6309297535714573\n",
      "\n",
      "Processing Debate ID: 14\n",
      "Ground truth scores: [0. 3. 0. 0. 0.]\n",
      "Predicted scores: [6.5 8.  6.  7.5 8.5]\n",
      "nDCG for this debate: 0.6309297535714573\n",
      "\n",
      "Processing Debate ID: 16\n",
      "Ground truth scores: [0. 1. 4. 0. 0.]\n",
      "Predicted scores: [6.5 7.  8.  6.  7.5]\n",
      "nDCG for this debate: 0.9717271130121371\n",
      "\n",
      "Processing Debate ID: 18\n",
      "Ground truth scores: [1. 1. 1. 0.]\n",
      "Predicted scores: [7.8 8.5 8.2 7.5]\n",
      "nDCG for this debate: 1.0\n",
      "\n",
      "Mean nDCG across all debates: 0.7853892771805698\n",
      "Mean nDCG (excluding debates 10 and 17): 0.7854\n"
     ]
    }
   ],
   "source": [
    "# Load the DataFrames\n",
    "self_assessment_df = pd.read_csv(\"rankings_self_assessment_.csv\")\n",
    "predictions_df = pd.read_csv(\"prompt1_gemini.csv\")\n",
    "\n",
    "# Exclude debates 10 and 17\n",
    "exclude_debates = [10, 17]\n",
    "\n",
    "# Calculate mean nDCG\n",
    "mean_ndcg = calculate_ndcg_from_dataframes(self_assessment_df, predictions_df, k=5, exclude_debates=exclude_debates)\n",
    "print(f\"Mean nDCG (excluding debates 10 and 17): {mean_ndcg:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean nDCG (excluding debates 10 and 17): 0.8987\n"
     ]
    }
   ],
   "source": [
    "# Load the DataFrames\n",
    "self_assessment_df = pd.read_csv(\"rankings_self_assessment_.csv\")\n",
    "predictions_df = pd.read_csv(\"prompt1_gpt4o.csv\")\n",
    "\n",
    "# Exclude debates 10 and 17\n",
    "exclude_debates = [10, 17]\n",
    "\n",
    "# Calculate mean nDCG\n",
    "mean_ndcg = calculate_ndcg_from_dataframes(self_assessment_df, predictions_df, k=5, exclude_debates=exclude_debates)\n",
    "print(f\"Mean nDCG (excluding debates 10 and 17): {mean_ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean nDCG (excluding debates 10 and 17): 0.9236\n"
     ]
    }
   ],
   "source": [
    "# Load the DataFrames\n",
    "self_assessment_df = pd.read_csv(\"rankings_self_assessment_.csv\")\n",
    "predictions_df = pd.read_csv(\"prompt2_gemini.csv\")\n",
    "\n",
    "# Exclude debates 10 and 17\n",
    "exclude_debates = [10, 17]\n",
    "\n",
    "# Calculate mean nDCG\n",
    "mean_ndcg = calculate_ndcg_from_dataframes(self_assessment_df, predictions_df, k=5, exclude_debates=exclude_debates)\n",
    "print(f\"Mean nDCG (excluding debates 10 and 17): {mean_ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean nDCG (excluding debates 10 and 17): 0.8869\n"
     ]
    }
   ],
   "source": [
    "# Load the DataFrames\n",
    "self_assessment_df = pd.read_csv(\"rankings_self_assessment_.csv\")\n",
    "predictions_df = pd.read_csv(\"prompt2_gpt4o.csv\")\n",
    "\n",
    "# Exclude debates 10 and 17\n",
    "exclude_debates = [10, 17]\n",
    "\n",
    "# Calculate mean nDCG\n",
    "mean_ndcg = calculate_ndcg_from_dataframes(self_assessment_df, predictions_df, k=5, exclude_debates=exclude_debates)\n",
    "print(f\"Mean nDCG (excluding debates 10 and 17): {mean_ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean nDCG (excluding debates 10 and 17): 0.8266\n"
     ]
    }
   ],
   "source": [
    "# Load the DataFrames\n",
    "self_assessment_df = pd.read_csv(\"rankings_self_assessment_.csv\")\n",
    "predictions_df = pd.read_csv(\"prompt3_gemini.csv\")\n",
    "\n",
    "# Exclude debates 10 and 17\n",
    "exclude_debates = [10, 17]\n",
    "\n",
    "# Calculate mean nDCG\n",
    "mean_ndcg = calculate_ndcg_from_dataframes(self_assessment_df, predictions_df, k=5, exclude_debates=exclude_debates)\n",
    "print(f\"Mean nDCG (excluding debates 10 and 17): {mean_ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean nDCG (excluding debates 10 and 17): 0.8680\n"
     ]
    }
   ],
   "source": [
    "# Load the DataFrames\n",
    "self_assessment_df = pd.read_csv(\"rankings_self_assessment_.csv\")\n",
    "predictions_df = pd.read_csv(\"prompt3_gpt4o.csv\")\n",
    "\n",
    "# Exclude debates 10 and 17\n",
    "exclude_debates = [10, 17]\n",
    "\n",
    "# Calculate mean nDCG\n",
    "mean_ndcg = calculate_ndcg_from_dataframes(self_assessment_df, predictions_df, k=5, exclude_debates=exclude_debates)\n",
    "print(f\"Mean nDCG (excluding debates 10 and 17): {mean_ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean nDCG (excluding debates 10 and 17): 0.8234\n"
     ]
    }
   ],
   "source": [
    "# Load the DataFrames\n",
    "self_assessment_df = pd.read_csv(\"rankings_self_assessment_.csv\")\n",
    "predictions_df = pd.read_csv(\"prompt4_gemini.csv\")\n",
    "\n",
    "# Exclude debates 10 and 17\n",
    "exclude_debates = [10, 17]\n",
    "\n",
    "# Calculate mean nDCG\n",
    "mean_ndcg = calculate_ndcg_from_dataframes(self_assessment_df, predictions_df, k=5, exclude_debates=exclude_debates)\n",
    "print(f\"Mean nDCG (excluding debates 10 and 17): {mean_ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean nDCG (excluding debates 10 and 17): 0.9497\n"
     ]
    }
   ],
   "source": [
    "# Load the DataFrames\n",
    "self_assessment_df = pd.read_csv(\"rankings_self_assessment_.csv\")\n",
    "predictions_df = pd.read_csv(\"prompt4_gpt4o.csv\")\n",
    "\n",
    "# Exclude debates 10 and 17\n",
    "exclude_debates = [10, 17]\n",
    "\n",
    "# Calculate mean nDCG\n",
    "mean_ndcg = calculate_ndcg_from_dataframes(self_assessment_df, predictions_df, k=5, exclude_debates=exclude_debates)\n",
    "print(f\"Mean nDCG (excluding debates 10 and 17): {mean_ndcg:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
